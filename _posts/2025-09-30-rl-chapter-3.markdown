---
layout: post
title:  "Reinforcement Learning: An Introduction - Chapter 3"
categories: rl textbook
---
{% include katex.html %}
Tension between applicability and mathematical tractability - tradeoff
## Agent
Learner and decision maker
## Environment
Things agent interacts with

Anything that the agent cannot changed arbitrarily
## State
Whatever information is available to the agent
## Rewards
Special numerical values that the agent tries to maximise
## Policy $\pi$
Mapping from state to probabilities of each action
The policy changes over time
## Reinforcement Learning
An abstraction of the problem of goal-directed learning from interaction

Any learning problem abstracted to 3 signals:
1. Signal to represent choices made by agent (actions)
2. Signal to represent basis for decision (states)
3. Signal to define agent's goal (rewards, which are always single numbers)
## Reward Hypothesis
Goals and purpose can be thought of as the maximisation of the expected value of cumulative sum of reward
## Reward Signal
Received scalar signal

Method to communicate *what* you want to achieve, not *how* you want to achieve it
# Returns
$G_t=R_{t+1}+R_{t+2}+R_{t+3}+...R_{T}$
## Episodic Tasks
Interactions break down naturally into subsequences
Episode ends in a terminal state and resets
Set of all nonterminal states - $\mathcal{S}$
Set of all states (with terminals) - $\mathcal{S}^+$
### Episodic Notation
$S_{t,i}$ 
$t$ - time step
$i$ - Episode number
But we don't really use $i$, more commonly written as $S_t$
## Continuing Tasks
No identifiable episodes
## Discounted Returns
$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...=\sum^\infty_{k=0}\gamma^kR_{t+k+1}$

$\gamma$ - Discount Rate
$0\le\gamma\le1$
### Pole-Balancing
Can be episodic OR continuing -- depends on how the task is defined

Episodic when: the episodes are when the pole falls, reward is +1 for every time step before failure

Continuing when: reward is -1 on each failure and 0 at all other times and discounting is applied, thus value is $\gamma ^T$ which incentivises moves to keep pole balanced
## Unified Return Notation
$G_t=\sum^{T-t-1}_{k=0}\gamma^kR_{t+k+1}$
Which allows for when $T=\infty$ or $\gamma=1$ (but not both)
# Markov Property
State signal that succeeds in returning all relevant information
### Independence of Path
It doesn't matter how that position and velocity came about, all that matters is the current state signal

Independent of its history

$Pr(R_{t+1}=r,S_{t+1}=s'|S_0,A_0,R_1,...S_{t-1},A_{t-1},S_t,A_t)$
If the state signal has the Markov property, this can be written as:
$p(s',r|s,a)=Pr(R_{t+1}=r,S_{t+1}=s'|S_t,A_t)$ for all $r,s',S_t,A_t$


Markov state provides equivalent predictions to full histories
# Markov Decisions Processes (MDP)
RL tasks that satisfy the Markov property are called Markov Decision Processes

MDP enables the computation of other environmental quantities:
## Expected reward
$r(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{r\in \mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)$
## State Transition Probabilities
$p(s'|s,a)=Pr(S_{t+1}=s'|S_t=s,A_t=a)=\sum_{r\in\mathcal{R}}p(s',r|s,a)$
### Expected Rewards to state-action-next-state triples
$r(s,a,s')=\mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s']=\cfrac{\sum_{r\in \mathcal{R}}r\times p(s',r|s,a)}{p(s'|s,a)}$
![[Pasted image 20250930040238.png]]

## Policy Vs Value
Policy: Mapping: state $\to$ probability
Value: Expected return when following policy $\pi$ from state $S$

## Value/Expected return
$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s]$
where $\mathbb{E}_\pi$ denotes expected value of a random variable given that the agent follows policy $\pi$
## Important Note
Value of terminal state, if any, is always 0

# Action-Value Function for policy $\pi$
Value of taking an action $q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]=\mathbb{E}_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a]$


$v_\pi$ and $q_\pi$ can be estimated from experience

## Making $v_\pi$ Recursive
$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$
$=\mathbb{E}_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s]$
$=\mathbb{E}_\pi[R_{t+1}+\sum^\infty_{k=0}\gamma^kR_{t+k+2}|S_t=s]$
$=\underset{a}\sum\pi(a,s)\underset{s'}\sum\underset{r}\sum p(s',r|s,a)[r+\gamma\mathbb{E}_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+2}|S_{t+1}=s']]$ since $\mathbb{E}_\pi()=\sum\text{Value}\times p(\text{Value})$ for all actions, states and rewards
$=\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$ - #vpibellmanequation Bellman equation for $v_\pi$
# Optimal Value Functions
## Better Policies
A policy $\pi$ is better than or equal to another policy if its expected return $\ge$ $\pi'$ s expected return for all states $s$
$\pi\ge\pi' \iff v_\pi(s)\ge v_{\pi'}(s)$ for all $s\in S$ 
## Optimal Policy $\pi^*$
Policy $\pi^*$ is optimal if it is better than all other policies

There may be more than one optimal policy

### Optimal State-Value Function $v_*$
All optimal policies share the same state-value function known as the Optimal state-Value function $v_*$
$v_*(s)=\underset{\pi}\max v_\pi(s)$ for all $s\in \mathcal{S}$ 

**Bellman optimality equation**

$v_*(s)=\underset{a\in\mathcal{A}(s)}\max q_\pi(s,a)$
$=\underset{a}\max\mathbb{E}_{\pi*}[G_t|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}_{\pi*}[\sum^\infty_{k=0} \gamma^kR_{t+k+1}|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}_{\pi*}[R_{t+1}+\gamma\sum^\infty_{k=0} \gamma^kR_{t+k+2}|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}|S_t=s,A_t=a)]$ #vstarbellmanequation
$=\underset{a\in\mathcal{A(s)}}\max{\underset{s',r}\sum} p(s',r|s,a)[r+\gamma v_*(s')]$ #vstarbellmanequation
### Optimal Action-Value Function $q_*$
$q_*(s,a)=\underset{\pi}\max q_\pi(s,a)$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$
$q_*(s,a)=\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]$

**Bellman optimality equation**
$q_*(s,a)=\mathbb{E}[R_{t+1}+\gamma\underset{a'}\max q_*(S_{t+1},a')|S_t=s,A_t=a]$
$=\underset{s',r}\sum p(s',r|s,a)[r+\gamma \underset{a'}\max q_*(s',a')]$ #qstarbellmanequation

## Bellman Optimality equations
$N$ Bellman equations for $N$ states

The Bellman optimality equation is a system of equations, one for each state

## Greedy $v_*$ 
Greedy policies with $v_*$ are optimal policies $\pi^*$

Greedy policy + $v_*$ = Optimal in the long-term

### Benefit of $q$
With $q(s,a)$, nothing is needed to be known about successor states and their values
# Exercises
**Exercise 3.1**
<u>Speaking with another person</u>
States:
- Listening
- Speaking
- Terminal state of the end of convo
Actions:
- Nodding
- Saying words
- Keeping / breaking eye contact
Rewards:
- Feeling good
- Other person feeling good
- Gaining info
<u>Football</u>
States:
- Winning
- Losing
- Drawing
- Game in play
- Game over
Actions:
- Scoring a goal
- Conceding goal
Rewards:
- Winning
<u>Eating</u>
States:
- Hungry
- Full
Actions:
- Putting food in mouth
- Swallowing
- Vomiting
Reward:
- Endorphins
**Exercise 3.2**
Problems where a certain decision ends the learning situation entirely. i.e. hardcore Minecraft -- really poor decisions have irreversible consequences
**Exercise 3.3**
It depends.

The level of granularity is a choice. I suppose the lower, the more immediate the rewards?
But it might be difficult to go to higher level objectives when being rewardsed for lower-level actions

It must be a free choice
**Exercise 3.4**
The return for all actions is -1, since there is no way to get a positive reward, and each action is non-discounted. In the continuous discounted formulation, the agent is incentivised to avoid the pole-falling and different actions will have different likelihoods, resulting in different rewards that are discounted. In this new formulation, each action will ultimately lead to a terminal state and is discounted, thus there is no incentive to prevent the pole from falling.
**Exercise 3.5**
The robot must be incentivised to explore and find the exit. Until the robot happens to explore and find the exit (which may never happen if the maze is too big), then it will never associate positive reward with exiting
**Exercise 3.6**
Makrov state is when a given state can accurately predict all future states and expected rewards.

If we had a complete history of the past, we could now what was occluded.

If we assume the first situation, is a full history, then with the second situation, if nothing had changed going into the 2nd day, it would be a Markov state.

But, we do not have enough information, we cannot see what is hidden and cannot predict the next states. Cannot be a Markov state.
**Exercise 3.7**

**Exercise 3.8**
$\underset{s',r}\sum p(s',r|s,a)[r+\underset{a'}\sum \pi(a'|s')\gamma q_\pi(s',a')]$
**Exercise 3.9**
$v_\pi(s)=\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$
Assuming each state has probability 1/4 and using $\gamma=0.9$:
$\frac 1 4 (0.9\times2.3+0.9\times-0.4+0.9\times0.4+0.9\times0.7)$
$=0.675\approx 0.7 \square$ 
**Exercise 3.10**
The signs are important. If all rewards were positive, it is possible that a strong enough positive association could be built with the wrong outcome if enforced over time.

$v_\pi(s)=\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$ - Bellman Equation

$v'_\pi(s)=\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+c+\gamma v'_\pi(s')]$
$v'_\pi(s)=\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+c+\gamma [v_\pi(s')+v_c]]$
$=[\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)][r+c+\gamma [v_\pi(s')+v_c]]$
$=[c+\gamma v_c]+\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$
$=v_\pi(s)+\underset{a}\sum\pi(a|s)\underset{s',r}p(s',r|s,a)[c+\gamma v_c]$
$=v_\pi(s)+c+\gamma v_c$ since $c$ and $\gamma v_c$ are constants that do not change over $a,s',r$
And we know $v'_\pi(s)=v_\pi(s)+v_c$ 
$\therefore v_\pi(s)+c+\gamma v_c=v_\pi(s)+v_c$
$\to c + \gamma v_c=v_c$
$\to (1-\gamma)v_c=c$
$\to v_c=\frac c {(1-\gamma)}$

**Exercise 3.11**
Longer routes are now more incentivised. If $c$ is large enough, there is a greater incentive to do the longest route possible if $c$ is greater than rewards pre-existing

**Exercise 3.12**
$v_\pi(s)=\pi(a|S_t=s)$
$\mathbb{E}_\pi(q_\pi(s,a|S_t=s,a=a_3)$
$\pi(a|s)q_\pi(s,a)$
**Exercise 3.13**
I'm sorry, I have so many other assignments to do. Think these are quite self explanatory (I was too lazy)
**Exercise 3.14**
**Exercise 3.15**
**Exercise 3.16**
**Exercise 3.17**
**Exercise 3.18**
**Exercise 3.19**
**Exercise 3.20**
**Exercise 3.21**
