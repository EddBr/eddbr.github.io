---
layout: post
title:  "Reinforcement Learning: An Introduction - Chapter 1"
categories: rl textbook
---

# RL
RL is focused on goal-directed learning
Learning how to map situations to actions to maximise the reward signal
## Closed Loop
Actions influence later inputs
## RL Problem Requirements
1. Closed-loop
2. Not have direct instructions about what action to take
3. Consequences of actions over extended periods of time

RL tries to maximise reward signal instead of trying to find hidden structure
RL | Supervised Learning | Unsupervised Learning - they are distinct
### Exploration/Exploitation Trade-off
# Subelements of RL system
## 1. Policy
Mapping from perceived state to actions
## 2. Reward signal
Defines the goal
Each time step, the environment sends to the reinforcement learning agent a single number, a **reward**
## 3. Value Function
Specifies what is good in long run
Value of a state is the total reward expected in future from that state

Reward is immediate, primary | Value is long-term, secondary
## 4. (optional) Model of environment
Model of environment
Mimics environment and leads to predictions about the environment
Models enable planning, which is deciding on a course of action by considering possible future situations
Model-free is trial-and-error learning
# Evolutionary Methods
Non-learning (not learning from environment)
Finds and utilises best policy

| Pros                                                                                                                      | Cons                     |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| Works well when either:<br>1. Small set of policies<br>2. Good policies easy to find<br>3. Lots of time for policy search | Ignore useful structures |
| Have advantages when agent cannot sense state of environment                                                              |                          |
## Policy Gradient Methods
Estimate direction parameters should be adjusted to most rapidly improve policy's performance

# Tic-Tac-Toe
## Assumptions
Opposition player is imperfect and sometimes blunders
## Consequences
Cannot use minimax, because that assumes the opponent is always correct
# Value Update Equation
$V(s)\leftarrow V(s) + \alpha[V(s')-V(s)]$
$\alpha$ - Step size parameter

Converges if $\alpha$ reduces over time
If $\alpha$ remains $>0$, it can adjust to an opponent that changes strategy over time
# Problems with Evolutionary Methods
Value functions methods allow individual states to be evaluated
Evolutionary is less granular
Evolutionary holds policy fixed, which means that it attributes same value to all moves
# RL History

## 3 Threads led to RL
1. Trial + error learning
2. Optimal control -  designing a controller to minimise measure of behaviour
3. Temporal difference methods - difference between temporally successive estimates of same quantity
# Curse of Dimensionality
Computational requirements grow exponentially with number of variables
# Animal Learning 
## Law of Effect
Strength of feeling $\propto$ Strength of learning
## Secondary Reinforcer
Stimulus that has been paired with a primary reinforcer such as food or pain
(Value, rather than reward)
# Credit Assignment Problem
How do you distribute success for many decisions?
# Exercises
**Exercise 1.1: Self-Play** 
Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing? 

It would continually invent new strategies to improve, as long as the step size parameter $\alpha>0$

**Exercise 1.2: Symmetries**
Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that  symmetrically equivalent positions should necessarily have the same value?

Conduct rotations to reveal symmetries. Can have fewer paths to make and thus fewer decisions

Symmetrically equivalent positions should necessarily have the same values as long as the imperfect opponent makes the same blunders in those positions

**Exercise 1.3: Greedy Play** Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Would it learn to play better, or worse, than a non-greedy player? What problems might occur?

Greedy player priorities reward over value, which is too short-term and will likely lose more often

**Exercise 1.4: Learning from Exploration** 
Suppose learning updates occurred  after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?

When we do not learn from exploratory, the probabilities will match the ground truth probabilities.
If we learn from exploratory, we are also learning from random, likely sub-optimal moves, which will add random error to the probabilities. Better to learn from exploratory IF we never stop making exploratory moves, as it accounts for these future errors

**Exercise 1.5: Other Improvements**
Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?

Initial values are set to 0.5, which is not accurate. For example, if opponent has 2 in a row, the probability of winning will be lower. Will learn faster as a result
Better ways to solve problem may be heuristic-based
