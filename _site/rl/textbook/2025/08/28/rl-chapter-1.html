<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning: An Introduction - Chapter 1 | Eddie’s Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reinforcement Learning: An Introduction - Chapter 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="RL RL is focused on goal-directed learning Learning how to map situations to actions to maximise the reward signal Closed Loop Actions influence later inputs RL Problem Requirements Closed-loop Not have direct instructions about what action to take Consequences of actions over extended periods of time" />
<meta property="og:description" content="RL RL is focused on goal-directed learning Learning how to map situations to actions to maximise the reward signal Closed Loop Actions influence later inputs RL Problem Requirements Closed-loop Not have direct instructions about what action to take Consequences of actions over extended periods of time" />
<link rel="canonical" href="http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html" />
<meta property="og:url" content="http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html" />
<meta property="og:site_name" content="Eddie’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-28T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning: An Introduction - Chapter 1" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-28T00:00:00-07:00","datePublished":"2025-08-28T00:00:00-07:00","description":"RL RL is focused on goal-directed learning Learning how to map situations to actions to maximise the reward signal Closed Loop Actions influence later inputs RL Problem Requirements Closed-loop Not have direct instructions about what action to take Consequences of actions over extended periods of time","headline":"Reinforcement Learning: An Introduction - Chapter 1","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html"},"url":"http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Eddie&apos;s Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Eddie&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reinforcement Learning: An Introduction - Chapter 1</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-08-28T00:00:00-07:00" itemprop="datePublished">Aug 28, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="rl">RL</h1>
<p>RL is focused on goal-directed learning
Learning how to map situations to actions to maximise the reward signal</p>
<h2 id="closed-loop">Closed Loop</h2>
<p>Actions influence later inputs</p>
<h2 id="rl-problem-requirements">RL Problem Requirements</h2>
<ol>
  <li>Closed-loop</li>
  <li>Not have direct instructions about what action to take</li>
  <li>Consequences of actions over extended periods of time</li>
</ol>

<p>RL tries to maximise reward signal instead of trying to find hidden structure
RL | Supervised Learning | Unsupervised Learning - they are distinct</p>
<h3 id="explorationexploitation-trade-off">Exploration/Exploitation Trade-off</h3>
<h1 id="subelements-of-rl-system">Subelements of RL system</h1>
<h2 id="1-policy">1. Policy</h2>
<p>Mapping from perceived state to actions</p>
<h2 id="2-reward-signal">2. Reward signal</h2>
<p>Defines the goal
Each time step, the environment sends to the reinforcement learning agent a single number, a <strong>reward</strong></p>
<h2 id="3-value-function">3. Value Function</h2>
<p>Specifies what is good in long run
Value of a state is the total reward expected in future from that state</p>

<p>Reward is immediate, primary | Value is long-term, secondary</p>
<h2 id="4-optional-model-of-environment">4. (optional) Model of environment</h2>
<p>Model of environment
Mimics environment and leads to predictions about the environment
Models enable planning, which is deciding on a course of action by considering possible future situations
Model-free is trial-and-error learning</p>
<h1 id="evolutionary-methods">Evolutionary Methods</h1>
<p>Non-learning (not learning from environment)
Finds and utilises best policy</p>

<p>| Pros                                                                                                                      | Cons                     |
| ————————————————————————————————————————- | ———————— |
| Works well when either:<br />1. Small set of policies<br />2. Good policies easy to find<br />3. Lots of time for policy search | Ignore useful structures |
| Have advantages when agent cannot sense state of environment                                                              |                          |</p>
<h2 id="policy-gradient-methods">Policy Gradient Methods</h2>
<p>Estimate direction parameters should be adjusted to most rapidly improve policy’s performance</p>

<h1 id="tic-tac-toe">Tic-Tac-Toe</h1>
<h2 id="assumptions">Assumptions</h2>
<p>Opposition player is imperfect and sometimes blunders</p>
<h2 id="consequences">Consequences</h2>
<p>Cannot use minimax, because that assumes the opponent is always correct</p>
<h1 id="value-update-equation">Value Update Equation</h1>
<p>$V(s)\leftarrow V(s) + \alpha[V(s’)-V(s)]$
$\alpha$ - Step size parameter</p>

<p>Converges if $\alpha$ reduces over time
If $\alpha$ remains $&gt;0$, it can adjust to an opponent that changes strategy over time</p>
<h1 id="problems-with-evolutionary-methods">Problems with Evolutionary Methods</h1>
<p>Value functions methods allow individual states to be evaluated
Evolutionary is less granular
Evolutionary holds policy fixed, which means that it attributes same value to all moves</p>
<h1 id="rl-history">RL History</h1>

<h2 id="3-threads-led-to-rl">3 Threads led to RL</h2>
<ol>
  <li>Trial + error learning</li>
  <li>Optimal control -  designing a controller to minimise measure of behaviour</li>
  <li>Temporal difference methods - difference between temporally successive estimates of same quantity
    <h1 id="curse-of-dimensionality">Curse of Dimensionality</h1>
    <p>Computational requirements grow exponentially with number of variables</p>
    <h1 id="animal-learning">Animal Learning</h1>
    <h2 id="law-of-effect">Law of Effect</h2>
    <p>Strength of feeling $\propto$ Strength of learning</p>
    <h2 id="secondary-reinforcer">Secondary Reinforcer</h2>
    <p>Stimulus that has been paired with a primary reinforcer such as food or pain
(Value, rather than reward)</p>
    <h1 id="credit-assignment-problem">Credit Assignment Problem</h1>
    <p>How do you distribute success for many decisions?</p>
    <h1 id="exercises">Exercises</h1>
    <p><strong>Exercise 1.1: Self-Play</strong> 
Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?</p>
  </li>
</ol>

<p>It would continually invent new strategies to improve, as long as the step size parameter $\alpha&gt;0$</p>

<p><strong>Exercise 1.2: Symmetries</strong>
Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that  symmetrically equivalent positions should necessarily have the same value?</p>

<p>Conduct rotations to reveal symmetries. Can have fewer paths to make and thus fewer decisions</p>

<p>Symmetrically equivalent positions should necessarily have the same values as long as the imperfect opponent makes the same blunders in those positions</p>

<p><strong>Exercise 1.3: Greedy Play</strong> Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Would it learn to play better, or worse, than a non-greedy player? What problems might occur?</p>

<p>Greedy player priorities reward over value, which is too short-term and will likely lose more often</p>

<p><strong>Exercise 1.4: Learning from Exploration</strong> 
Suppose learning updates occurred  after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?</p>

<p>When we do not learn from exploratory, the probabilities will match the ground truth probabilities.
If we learn from exploratory, we are also learning from random, likely sub-optimal moves, which will add random error to the probabilities. Better to learn from exploratory IF we never stop making exploratory moves, as it accounts for these future errors</p>

<p><strong>Exercise 1.5: Other Improvements</strong>
Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?</p>

<p>Initial values are set to 0.5, which is not accurate. For example, if opponent has 2 in a row, the probability of winning will be lower. Will learn faster as a result
Better ways to solve problem may be heuristic-based</p>

  </div><a class="u-url" href="/rl/textbook/2025/08/28/rl-chapter-1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eddie&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Eddie&#39;s Blog</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/eddbr"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">eddbr</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Eddie&#39;s Blog. Looking at intersection of tech and people.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
