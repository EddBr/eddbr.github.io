<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-11-24T11:04:10+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eddie’s Blog</title><subtitle>4th Year CS Student | ML/RL Research | Founder of ProcrastApply</subtitle><entry><title type="html">Identity Amplification CHI ‘26 Submission</title><link href="http://localhost:4000/research/2025/10/15/identity-amplification.html" rel="alternate" type="text/html" title="Identity Amplification CHI ‘26 Submission" /><published>2025-10-15T11:00:00+01:00</published><updated>2025-10-15T11:00:00+01:00</updated><id>http://localhost:4000/research/2025/10/15/identity-amplification</id><content type="html" xml:base="http://localhost:4000/research/2025/10/15/identity-amplification.html"><![CDATA[<p>I’m publishing the full PDF of our CHI 2026 submission, <strong>Identity Amplification</strong>, so it is easy to reference directly from the blog.</p>

<div class="pdf-viewer">
  <object data="/assets/pdfs/Identity_Amplification_CHI_26_Submission.pdf" type="application/pdf" width="100%" height="800px">
    <p>Your browser can't display PDFs inline, but you can <a href="/assets/pdfs/Identity_Amplification_CHI_26_Submission.pdf" target="_blank">download the paper here</a>.</p>
  </object>
</div>

<style>
.pdf-viewer {
  margin: 2rem 0;
  border: 1px solid #e5e7eb;
  border-radius: 8px;
  overflow: hidden;
  box-shadow: 0 4px 12px rgba(0,0,0,0.08);
}
</style>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Sharing the Identity Amplification CHI submission as a readable PDF.]]></summary></entry><entry><title type="html">Reinforcement Learning: An Introduction - Chapter 3</title><link href="http://localhost:4000/rl/textbook/2025/09/30/rl-chapter-3.html" rel="alternate" type="text/html" title="Reinforcement Learning: An Introduction - Chapter 3" /><published>2025-09-30T00:00:00+01:00</published><updated>2025-09-30T00:00:00+01:00</updated><id>http://localhost:4000/rl/textbook/2025/09/30/rl-chapter-3</id><content type="html" xml:base="http://localhost:4000/rl/textbook/2025/09/30/rl-chapter-3.html"><![CDATA[<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body,
	      {delimiters: [
	      {left: '$$', right: '$$', display: true},
	      {left: '$', right: '$', display: false},
	      {left: '\\(', right: '\\)', display: false},
	      {left: '\\[', right: '\\]', display: true}
	      ]});"></script>

<p>Tension between applicability and mathematical tractability - tradeoff</p>
<h2 id="agent">Agent</h2>
<p>Learner and decision maker</p>
<h2 id="environment">Environment</h2>
<p>Things agent interacts with</p>

<p>Anything that the agent cannot changed arbitrarily</p>
<h2 id="state">State</h2>
<p>Whatever information is available to the agent</p>
<h2 id="rewards">Rewards</h2>
<p>Special numerical values that the agent tries to maximise</p>
<h2 id="policy-pi">Policy $\pi$</h2>
<p>Mapping from state to probabilities of each action
The policy changes over time</p>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>An abstraction of the problem of goal-directed learning from interaction</p>

<p>Any learning problem abstracted to 3 signals:</p>
<ol>
  <li>Signal to represent choices made by agent (actions)</li>
  <li>Signal to represent basis for decision (states)</li>
  <li>Signal to define agent’s goal (rewards, which are always single numbers)
    <h2 id="reward-hypothesis">Reward Hypothesis</h2>
    <p>Goals and purpose can be thought of as the maximisation of the expected value of cumulative sum of reward</p>
    <h2 id="reward-signal">Reward Signal</h2>
    <p>Received scalar signal</p>
  </li>
</ol>

<p>Method to communicate <em>what</em> you want to achieve, not <em>how</em> you want to achieve it</p>
<h1 id="returns">Returns</h1>
<p>$G_t=R_{t+1}+R_{t+2}+R_{t+3}+…R_{T}$</p>
<h2 id="episodic-tasks">Episodic Tasks</h2>
<p>Interactions break down naturally into subsequences
Episode ends in a terminal state and resets
Set of all nonterminal states - $\mathcal{S}$
Set of all states (with terminals) - $\mathcal{S}^+$</p>
<h3 id="episodic-notation">Episodic Notation</h3>
<p>$S_{t,i}$ 
$t$ - time step
$i$ - Episode number
But we don’t really use $i$, more commonly written as $S_t$</p>
<h2 id="continuing-tasks">Continuing Tasks</h2>
<p>No identifiable episodes</p>
<h2 id="discounted-returns">Discounted Returns</h2>
<p>$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+…=\sum^\infty_{k=0}\gamma^kR_{t+k+1}$</p>

<p>$\gamma$ - Discount Rate
$0\le\gamma\le1$</p>
<h3 id="pole-balancing">Pole-Balancing</h3>
<p>Can be episodic OR continuing – depends on how the task is defined</p>

<p>Episodic when: the episodes are when the pole falls, reward is +1 for every time step before failure</p>

<p>Continuing when: reward is -1 on each failure and 0 at all other times and discounting is applied, thus value is $\gamma ^T$ which incentivises moves to keep pole balanced</p>
<h2 id="unified-return-notation">Unified Return Notation</h2>
<p>$G_t=\sum^{T-t-1}<em>{k=0}\gamma^kR</em>{t+k+1}$
Which allows for when $T=\infty$ or $\gamma=1$ (but not both)</p>
<h1 id="markov-property">Markov Property</h1>
<p>State signal that succeeds in returning all relevant information</p>
<h3 id="independence-of-path">Independence of Path</h3>
<p>It doesn’t matter how that position and velocity came about, all that matters is the current state signal</p>

<p>Independent of its history</p>

<p>$Pr(R_{t+1}=r,S_{t+1}=s’|S_0,A_0,R_1,…S_{t-1},A_{t-1},S_t,A_t)$
If the state signal has the Markov property, this can be written as:
$p(s’,r|s,a)=Pr(R_{t+1}=r,S_{t+1}=s’|S_t,A_t)$ for all $r,s’,S_t,A_t$</p>

<p>Markov state provides equivalent predictions to full histories</p>
<h1 id="markov-decisions-processes-mdp">Markov Decisions Processes (MDP)</h1>
<p>RL tasks that satisfy the Markov property are called Markov Decision Processes</p>

<p>MDP enables the computation of other environmental quantities:</p>
<h2 id="expected-reward">Expected reward</h2>
<p>$r(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{r\in \mathcal{R}}r\sum_{s’\in\mathcal{S}}p(s’,r|s,a)$</p>
<h2 id="state-transition-probabilities">State Transition Probabilities</h2>
<p>$p(s’|s,a)=Pr(S_{t+1}=s’|S_t=s,A_t=a)=\sum_{r\in\mathcal{R}}p(s’,r|s,a)$</p>
<h3 id="expected-rewards-to-state-action-next-state-triples">Expected Rewards to state-action-next-state triples</h3>
<p>$r(s,a,s’)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s’]=\cfrac{\sum_{r\in \mathcal{R}}r\times p(s’,r|s,a)}{p(s’|s,a)}$
![[Pasted image 20250930040238.png]]</p>

<h2 id="policy-vs-value">Policy Vs Value</h2>
<p>Policy: Mapping: state $\to$ probability
Value: Expected return when following policy $\pi$ from state $S$</p>

<h2 id="valueexpected-return">Value/Expected return</h2>
<p>$v_\pi(s)=\mathbb{E}<em>\pi[G_t|S_t=s]=\mathbb{E}</em>\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s]$
where $\mathbb{E}_\pi$ denotes expected value of a random variable given that the agent follows policy $\pi$</p>
<h2 id="important-note">Important Note</h2>
<p>Value of terminal state, if any, is always 0</p>

<h1 id="action-value-function-for-policy-pi">Action-Value Function for policy $\pi$</h1>
<p>Value of taking an action $q_\pi(s,a)=\mathbb{E}<em>\pi[G_t|S_t=s,A_t=a]=\mathbb{E}</em>\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a]$</p>

<p>$v_\pi$ and $q_\pi$ can be estimated from experience</p>

<h2 id="making-v_pi-recursive">Making $v_\pi$ Recursive</h2>
<p>$v_\pi(s)=\mathbb{E}<em>\pi[G_t|S_t=s]$
$=\mathbb{E}</em>\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s]$
$=\mathbb{E}<em>\pi[R</em>{t+1}+\sum^\infty_{k=0}\gamma^kR_{t+k+2}|S_t=s]$
$=\underset{a}\sum\pi(a,s)\underset{s’}\sum\underset{r}\sum p(s’,r|s,a)[r+\gamma\mathbb{E}<em>\pi[\sum^\infty</em>{k=0}\gamma^kR_{t+k+2}|S_{t+1}=s’]]$ since $\mathbb{E}<em>\pi()=\sum\text{Value}\times p(\text{Value})$ for all actions, states and rewards
$=\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[r+\gamma v</em>\pi(s’)]$ - #vpibellmanequation Bellman equation for $v_\pi$</p>
<h1 id="optimal-value-functions">Optimal Value Functions</h1>
<h2 id="better-policies">Better Policies</h2>
<p>A policy $\pi$ is better than or equal to another policy if its expected return $\ge$ $\pi’$ s expected return for all states $s$
$\pi\ge\pi’ \iff v_\pi(s)\ge v_{\pi’}(s)$ for all $s\in S$</p>
<h2 id="optimal-policy-pi">Optimal Policy $\pi^*$</h2>
<p>Policy $\pi^*$ is optimal if it is better than all other policies</p>

<p>There may be more than one optimal policy</p>

<h3 id="optimal-state-value-function-v_">Optimal State-Value Function $v_*$</h3>
<p>All optimal policies share the same state-value function known as the Optimal state-Value function $v_<em>$
$v_</em>(s)=\underset{\pi}\max v_\pi(s)$ for all $s\in \mathcal{S}$</p>

<p><strong>Bellman optimality equation</strong></p>

<p>$v_<em>(s)=\underset{a\in\mathcal{A}(s)}\max q_\pi(s,a)$
$=\underset{a}\max\mathbb{E}_{\pi</em>}[G_t|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}<em>{\pi*}[\sum^\infty</em>{k=0} \gamma^kR_{t+k+1}|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}<em>{\pi*}[R</em>{t+1}+\gamma\sum^\infty_{k=0} \gamma^kR_{t+k+2}|S_t=s,A_t=a]$
$=\underset{a}\max\mathbb{E}[R_{t+1}+\gamma v_<em>(S_{t+1}|S_t=s,A_t=a)]$ #vstarbellmanequation
$=\underset{a\in\mathcal{A(s)}}\max{\underset{s’,r}\sum} p(s’,r|s,a)[r+\gamma v_</em>(s’)]$ #vstarbellmanequation</p>
<h3 id="optimal-action-value-function-q_">Optimal Action-Value Function $q_*$</h3>
<p>$q_<em>(s,a)=\underset{\pi}\max q_\pi(s,a)$ for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$
$q_</em>(s,a)=\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]$</p>

<p><strong>Bellman optimality equation</strong>
$q_<em>(s,a)=\mathbb{E}[R_{t+1}+\gamma\underset{a’}\max q_</em>(S_{t+1},a’)|S_t=s,A_t=a]$
$=\underset{s’,r}\sum p(s’,r|s,a)[r+\gamma \underset{a’}\max q_*(s’,a’)]$ #qstarbellmanequation</p>

<h2 id="bellman-optimality-equations">Bellman Optimality equations</h2>
<p>$N$ Bellman equations for $N$ states</p>

<p>The Bellman optimality equation is a system of equations, one for each state</p>

<h2 id="greedy-v_">Greedy $v_*$</h2>
<p>Greedy policies with $v_<em>$ are optimal policies $\pi^</em>$</p>

<p>Greedy policy + $v_*$ = Optimal in the long-term</p>

<h3 id="benefit-of-q">Benefit of $q$</h3>
<p>With $q(s,a)$, nothing is needed to be known about successor states and their values</p>
<h1 id="exercises">Exercises</h1>
<p><strong>Exercise 3.1</strong>
<u>Speaking with another person</u>
States:</p>
<ul>
  <li>Listening</li>
  <li>Speaking</li>
  <li>Terminal state of the end of convo
Actions:</li>
  <li>Nodding</li>
  <li>Saying words</li>
  <li>Keeping / breaking eye contact
Rewards:</li>
  <li>Feeling good</li>
  <li>Other person feeling good</li>
  <li>Gaining info
<u>Football</u>
States:</li>
  <li>Winning</li>
  <li>Losing</li>
  <li>Drawing</li>
  <li>Game in play</li>
  <li>Game over
Actions:</li>
  <li>Scoring a goal</li>
  <li>Conceding goal
Rewards:</li>
  <li>Winning
<u>Eating</u>
States:</li>
  <li>Hungry</li>
  <li>Full
Actions:</li>
  <li>Putting food in mouth</li>
  <li>Swallowing</li>
  <li>Vomiting
Reward:</li>
  <li>Endorphins
<strong>Exercise 3.2</strong>
Problems where a certain decision ends the learning situation entirely. i.e. hardcore Minecraft – really poor decisions have irreversible consequences
<strong>Exercise 3.3</strong>
It depends.</li>
</ul>

<p>The level of granularity is a choice. I suppose the lower, the more immediate the rewards?
But it might be difficult to go to higher level objectives when being rewardsed for lower-level actions</p>

<p>It must be a free choice
<strong>Exercise 3.4</strong>
The return for all actions is -1, since there is no way to get a positive reward, and each action is non-discounted. In the continuous discounted formulation, the agent is incentivised to avoid the pole-falling and different actions will have different likelihoods, resulting in different rewards that are discounted. In this new formulation, each action will ultimately lead to a terminal state and is discounted, thus there is no incentive to prevent the pole from falling.
<strong>Exercise 3.5</strong>
The robot must be incentivised to explore and find the exit. Until the robot happens to explore and find the exit (which may never happen if the maze is too big), then it will never associate positive reward with exiting
<strong>Exercise 3.6</strong>
Makrov state is when a given state can accurately predict all future states and expected rewards.</p>

<p>If we had a complete history of the past, we could now what was occluded.</p>

<p>If we assume the first situation, is a full history, then with the second situation, if nothing had changed going into the 2nd day, it would be a Markov state.</p>

<p>But, we do not have enough information, we cannot see what is hidden and cannot predict the next states. Cannot be a Markov state.
<strong>Exercise 3.7</strong></p>

<p><strong>Exercise 3.8</strong>
$\underset{s’,r}\sum p(s’,r|s,a)[r+\underset{a’}\sum \pi(a’|s’)\gamma q_\pi(s’,a’)]$
<strong>Exercise 3.9</strong>
$v_\pi(s)=\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[r+\gamma v_\pi(s’)]$
Assuming each state has probability 1/4 and using $\gamma=0.9$:
$\frac 1 4 (0.9\times2.3+0.9\times-0.4+0.9\times0.4+0.9\times0.7)$
$=0.675\approx 0.7 \square$ 
<strong>Exercise 3.10</strong>
The signs are important. If all rewards were positive, it is possible that a strong enough positive association could be built with the wrong outcome if enforced over time.</p>

<table>
  <tbody>
    <tr>
      <td>$v_\pi(s)=\underset{a}\sum\pi(a</td>
      <td>s)\underset{s’,r}p(s’,r</td>
      <td>s,a)[r+\gamma v_\pi(s’)]$ - Bellman Equation</td>
    </tr>
  </tbody>
</table>

<p>$v’<em>\pi(s)=\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[r+c+\gamma v’</em>\pi(s’)]$
$v’<em>\pi(s)=\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[r+c+\gamma [v</em>\pi(s’)+v_c]]$
$=[\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)][r+c+\gamma [v_\pi(s’)+v_c]]$
$=[c+\gamma v_c]+\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[r+\gamma v_\pi(s’)]$
$=v_\pi(s)+\underset{a}\sum\pi(a|s)\underset{s’,r}p(s’,r|s,a)[c+\gamma v_c]$
$=v_\pi(s)+c+\gamma v_c$ since $c$ and $\gamma v_c$ are constants that do not change over $a,s’,r$
And we know $v’<em>\pi(s)=v</em>\pi(s)+v_c$ 
$\therefore v_\pi(s)+c+\gamma v_c=v_\pi(s)+v_c$
$\to c + \gamma v_c=v_c$
$\to (1-\gamma)v_c=c$
$\to v_c=\frac c {(1-\gamma)}$</p>

<p><strong>Exercise 3.11</strong>
Longer routes are now more incentivised. If $c$ is large enough, there is a greater incentive to do the longest route possible if $c$ is greater than rewards pre-existing</p>

<p><strong>Exercise 3.12</strong>
$v_\pi(s)=\pi(a|S_t=s)$
$\mathbb{E}<em>\pi(q</em>\pi(s,a|S_t=s,a=a_3)$
$\pi(a|s)q_\pi(s,a)$
<strong>Exercise 3.13</strong>
I’m sorry, I have so many other assignments to do. Think these are quite self explanatory (I was too lazy)
<strong>Exercise 3.14</strong>
<strong>Exercise 3.15</strong>
<strong>Exercise 3.16</strong>
<strong>Exercise 3.17</strong>
<strong>Exercise 3.18</strong>
<strong>Exercise 3.19</strong>
<strong>Exercise 3.20</strong>
<strong>Exercise 3.21</strong></p>]]></content><author><name></name></author><category term="rl" /><category term="textbook" /><summary type="html"><![CDATA[Tension between applicability and mathematical tractability - tradeoff Agent Learner and decision maker Environment Things agent interacts with]]></summary></entry><entry><title type="html">LLMs in the World</title><link href="http://localhost:4000/llm/communication/2025/09/12/llms-in-the-world.html" rel="alternate" type="text/html" title="LLMs in the World" /><published>2025-09-12T00:00:00+01:00</published><updated>2025-09-12T00:00:00+01:00</updated><id>http://localhost:4000/llm/communication/2025/09/12/llms-in-the-world</id><content type="html" xml:base="http://localhost:4000/llm/communication/2025/09/12/llms-in-the-world.html"><![CDATA[<p>LLMs are becoming the new social media. The problems we are and will encounter, the benefits and the impact on human social function. Much the same way as people now incorporate videos, photos, and Wikipedia pages as verification of their own thoughts, or enhancements to the conversation being engaged in, LLMs will take a new step. Emotions will be more effectively conveyed and we will begin to question how we ever operated without them. People will use it as a sounding board for their ideas as well as a tool to more effectively communicate themselves. Communication is a 2 way street:
In 1 direction you have the communicator
In the other you have the listener
And this switches.</p>

<p>However, there is a contextual disconnect between the idea that the listener is hearing and what the communicator is trying to convey. What seems obvious or what may cause an emotional reaction or what emotion is evoked depends on the person. To some, shouting may indicate importance, to others it means loss of control and thus insignificant. Same words in the same tone, but different interpretations. This is miscommunication.</p>

<p>LLMs offer a novel way to think outside of one’s own sphere without criticism, there are subtle ways to make a person think anew.</p>

<p>The issue is that current LLMs mirror the listener and encourage an echo chamber of one’s own thoughts. This is BAD. No good will come of this. People will get trapped further down their own personalised rabbit holes and narratives.</p>

<p>“I think this person is saying this because they’re always just in a bad mood.” -&gt; Should lead to a question or nudge such as, “Why do you think they’re in a bad mood?”
But instead will likely lead to “That sounds tough, they are always such a downer.”
I hope the trajectory changes because this technology really could offer a chance for new human-insights. Communication education impacts people’s lives and can have positive change. Companies are profit-motivated, how do we make this design decision profitable?</p>

<p>I think I’m obsessed with getting the most effective outcome from a situation and good communication is a really good method to do so. Everyone can have more from these situations.</p>

<p>I suppose this is where the cheating at life thing comes in, but I don’t think it can work. It doesn’t fix the problems LLMs engender, merely exacerbates and expands their sycophantic reach.</p>]]></content><author><name></name></author><category term="LLM" /><category term="communication" /><summary type="html"><![CDATA[LLMs are becoming the new social media. The problems we are and will encounter, the benefits and the impact on human social function. Much the same way as people now incorporate videos, photos, and Wikipedia pages as verification of their own thoughts, or enhancements to the conversation being engaged in, LLMs will take a new step. Emotions will be more effectively conveyed and we will begin to question how we ever operated without them. People will use it as a sounding board for their ideas as well as a tool to more effectively communicate themselves. Communication is a 2 way street: In 1 direction you have the communicator In the other you have the listener And this switches.]]></summary></entry><entry><title type="html">Reinforcement Learning: An Introduction - Chapter 2</title><link href="http://localhost:4000/rl/textbook/2025/09/01/rl-chapter-2.html" rel="alternate" type="text/html" title="Reinforcement Learning: An Introduction - Chapter 2" /><published>2025-09-01T00:00:00+01:00</published><updated>2025-09-01T00:00:00+01:00</updated><id>http://localhost:4000/rl/textbook/2025/09/01/rl-chapter-2</id><content type="html" xml:base="http://localhost:4000/rl/textbook/2025/09/01/rl-chapter-2.html"><![CDATA[<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body,
	      {delimiters: [
	      {left: '$$', right: '$$', display: true},
	      {left: '$', right: '$', display: false},
	      {left: '\\(', right: '\\)', display: false},
	      {left: '\\[', right: '\\]', display: true}
	      ]});"></script>

<h1 id="instructive-feedback">Instructive Feedback</h1>
<p>Indicates correct action to take
Independent of action taken</p>
<h1 id="evaluative-feedback">Evaluative Feedback</h1>
<p>Indicates how good action taken is
Depends entirely on action</p>
<h2 id="associative">Associative</h2>
<p>Actions are taken in more than 1 situation</p>
<h1 id="an-n-armed-bandit-problem">AN $n$-Armed Bandit Problem</h1>
<p>Limited time steps
Imagine an $n$-armed slot machine</p>

<p>Not choosing the greedy move is exploration
Choosing greedy is exploitation</p>
<h1 id="action-value">Action Value</h1>
<h2 id="true-value-qa">True Value $q(a)$</h2>
<p>Mean reward received when action is selected</p>
<h2 id="estimated-value-q_ta">Estimated Value $Q_t(a)$</h2>
<p>$Q_t(a)=\cfrac{R_1+R_2+…+R_{N_t(a)}}{N_t(a)}$
$N_t(a)$ - Number of times action $a$ is selected
$t$ - Time step
$R_t$ - Reward</p>

<p>If $N_t(a)=0$, we define $Q_t(a)$ with a default value. E.g. $Q_1(a)=0$</p>

<p>$\lim_{N_t(a)\to\infty}Q_t(a)\to q(a)$</p>

<h2 id="greedy-selection">Greedy Selection</h2>
<p>$A_t=\arg\max_aQ_t(a)$</p>

<p>$\epsilon$-greed algorithms are guaranteed to converge
Probability of selecting optimal action converges to $1-\epsilon$</p>
<h2 id="nonstationary">Nonstationary</h2>
<p>True values changed over time
$e$-greedy are necessary</p>

<p>There is a problem with this, however, as $N_t(a)$ increases, so do the memory requirements, without bound
Can instead be computed:
$Q_{k+1}=\frac 1 k \sum^k_{i=1}R_i$
$=\frac 1 k (R_k+\sum^{k-1}_{i=1}R_i)$
$=\frac 1 k (R_k+(k-1)Q_k+(Q_k-Q_k))$
$=\frac 1 k (R_k+kQ_k-Q_k)$
$=Q_k+\frac 1 k (R_k-Q_k)$</p>

<p>For me, it’s more intuitive to think of it as:
$Q_{k+1}=\cfrac{R_k+(k-1)Q_k}{k}$</p>
<h2 id="update-rule">Update Rule</h2>
<p>$NewEstimate\leftarrow OldEstimate+StepSize[Target-OldEstimate]$
which is like gradient descent ($W_k=W_{k-1}-\alpha\nabla W_{k-1}$)</p>

<p>Note that for us, the step size $\alpha=\frac 1 k$ is changing over time</p>
<h2 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h2>
<p>$Q_{k+1}=Q_k+\alpha[R_k-Q_k]$
So the weighted average past rewards and initial estimate $Q_1$ is:
$Q_{k+1}=Q_k+\alpha[R_k-Q_k]$
$=\alpha R_k+(1-\alpha)Q_k$
$=\alpha R_k+(1-\alpha)Q_k$
$=\alpha R_k+(1-\alpha)[\alpha R_{k-1}+(1-\alpha)Q_{k-1}]$
$=\alpha R_k+(1-\alpha)\alpha R_{k-1}+(1-\alpha)^2Q_{k-1}$
$=\alpha R_k+(1-\alpha)\alpha R_{k-1}+(1-\alpha)^2Q_{k-1}+…+(1-\alpha)^{k-1}\alpha R_1+(1-\alpha)^kQ_1$
$=(1-\alpha)^kQ_1+\sum^k_{i=1}\alpha(1-\alpha)^{k-i}R_i$</p>

<p>This is a weighted average because the sum of the weights is $(1-\alpha)^k+\sum^k_{i=1}\alpha(1-\alpha)^{k-i}=1$ #nonstationaryweightedequ
<strong>Proof:</strong>
Geometric sum equation: $S_n=\cfrac {a(1-r^n)}{1-r}$
Looking at $\sum^k_{i=1}\alpha(1-\alpha)^{k-i}$:
$(1-\alpha)^{-1}&gt;1$ since $0&lt;\alpha\le1$, and for this section, we’ll assume $alpha&lt;1$ and will deal with this later as an edge case
Therefore, we can reverse the direction of the sum, setting $r$ as $(1-\alpha)$
$\therefore S_n=\cfrac {\alpha(1-(1-\alpha)^k)}{1-(1-\alpha)}$ 
$=\cfrac {\alpha(1-(1-\alpha)^k)}{\alpha}$
$=(1-(1-\alpha)^k)$</p>

<p>If we add this back to #nonstationaryweightedequ 
$(1-\alpha)^k+(1-(1-\alpha)^k)$
$=1$ $\square$</p>

<p>At the edge case, when $\alpha=1$
The first part of #nonstationaryweightedequ $ becomes 0
In the second part, every part of the sum becomes 0, except the last term, $\alpha(1-\alpha)^{k-k}=\alpha(1-\alpha)^0=\alpha=1$
Therefore, this also sums to 1, $\square$</p>
<h2 id="convergence-conditions">Convergence Conditions</h2>
<p>$\sum^\infty_{k=1}\alpha_k(a)=\infty$
$\sum^\infty_{k=1}\alpha^2_k(a)&lt;\infty$</p>

<p>Does converge for sample-average case $\alpha_k(a)=\frac 1 k$
This means that it does not converge when $\alpha$ is constant
Non-convergence is desirable for a nonstationary environment</p>

<p>Step-size parameters that meet the convergence conditions converge very slowly and need a lot of training - rarely ideal</p>
<h2 id="optimistic-initial-values">Optimistic Initial Values</h2>
<p>Setting expected values much higher encourages exploration
However, the drive to explore is temporary and thus doesn’t work well in nonstationary problems (stops exploring, 0 adaptation)</p>
<h1 id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h1>
<p>$A_t=\arg\max_a[Q_t(a)+c\sqrt{\cfrac{\ln t}{N_t(a)}}]$
$c\sqrt{\cfrac{\ln t}{N_t(a)}}$ - Measure of uncertainty in estimate of $q(a)$</p>

<p>We use UCB because $\epsilon$-greedy indiscriminately chooses non-greedy options</p>

<p>Each time $a$ is chosen, $N_t(a)$ increases and thus reduces uncertainty</p>

<p>When $a$ is not selected, $t$ increases, increasing uncertainty</p>

<p>Explanation for spike in reward with UCB: It gives a large weighting to a route it estimates to have high value since $c=2$, it may be greater than $Q_t(a)$</p>
<h1 id="gradient-bandits">Gradient Bandits</h1>
<p>$\Pr{A-t=a}=\cfrac {e^{H_t(a)}}{\sum^n_{b=1}e^{H_t(b)}}=\pi_t(a)$</p>

<p>$H_{t+1}(A_t)=H_t(A_t)+\alpha(R_t-\bar R_t)(1-\pi_t(A_t)),$ #gradientbanditequation
$H_{t+1}(a)=H_t(a)+\alpha(R_t-\bar R_t)\pi_t(a)$
$\bar R$ is average of all rewards, serves as a baseline to compare $R_t$ to
If reward is higher than baseline ($R_t&gt;\bar R_t$), probability of choosing that action increases</p>

<p>Having a baseline means that a shifting of rewards has no bearing on performance (i.e. if all rewards are set to +4, rather than 0)</p>

<p>$H_{t+1}(a)=H_t(a)+\alpha\cfrac {\partial \mathbb{E} (R_t)}{\partial H_t(a)}$
Measure of of performance is the expected reward $\mathbb{E}(R_t)=\sum_b\pi_t(b)q(b)$</p>

<p>This algorithm is an instance of stochastic gradient ascent</p>

<p>$\cfrac {\partial \mathbb{E} (R_t)}{\partial H_t(a)}=\cfrac {\partial }{\partial H_t(a)}[\sum_b\pi_t(b)q(b)]$
$=\sum_bq(b)\cfrac{\partial \pi_t(b)}{\partial H_t(a)}$
$=\sum_b(q(b)-X_t)\cfrac{\partial \pi_t(b)}{\partial H_t(a)}$
$X_t$ - any scalar that does not depend on $b$
$b$ is some action in the action space
Can include $X_t$ because the gradient sums to 0 over all actions (sum of probabilities must remain 1, so sum of changes must $= 0$)</p>

<p>$=\sum_b\pi_t(b)(q(b)-X_t)\cfrac{\partial pi_t(b)}{\partial H_t(a)}/\pi_t(b)$
The equation is now in the form of an expectation, summing over all possible values $b$ of the random variable $A_t$ then multiplying by the probability of those values
$=\mathbb{E}[(q(A_t)-X_t)\cfrac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(A_t)]$
$=\mathbb{E}[(R_t-\bar R_t)\cfrac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(A_t)]$
We set $X_t=\bar R_t$ because it can equal anything
$R_t=q(A_t)$ is permitted because $\mathbb{E}[R_t]=q(A_t)$ and all other factors are non-random</p>

<p>Assume $\cfrac{\partial \pi_t(b)}{\partial H_t(a)}=\pi_t(b)(\mathbb{I}_{a=A_t}-\pi_t(a))$:</p>

<p>$=\mathbb{E}[(R_t-\bar R_t)\pi_t(A_t)(\mathbb{I}<em>{a=A_t}-\pi_t(a))/\pi_t(A_t)]$
$=\mathbb{E}[(R_t-\bar R_t)(\mathbb{I}</em>{a=A_t}-\pi_t(a))]$</p>

<p>where $\mathbb{I}_{a=b}=1$ if $a=b$
otherwise $\mathbb{I}=0$</p>

<p>which fits into the #gradientbanditequation :
$H_{t+1}(a)=H_t(a)+\alpha(R_t-\bar R_t)(\mathbb{I}_{a=A_t}-\pi_t(a))$</p>

<p>Remains to show  $\cfrac{\partial \pi_t(b)}{\partial H_t(a)}=\pi_t(b)(\mathbb{I}_{a=A_t}-\pi_t(a))$:
We begin with the quotient rule equation: $\cfrac {\partial}{\partial x}[\cfrac {f(x)}{g(x)}]=\cfrac {\frac {\partial f(x)}{\partial x}g(x)-f(x)\frac {\partial g(x)}{\partial x}}{g(x)^2}$
From this we can write:</p>

<p>$\cfrac{\partial \pi_t(b)}{\partial H_t(a)}=\cfrac{\partial}{\partial H_t(a)}\pi_t(b)$
$=\cfrac {\partial}{\partial H_t(a)}[\cfrac {e^{H_t(b)}}{\sum^n_{c=1}e^{H_t(c)}}]$
$=\cfrac {\frac {\partial e^{H_t(b)}}{\partial H_t(a)}\sum^n_{c=1}e^{H_t(c)}-e^{H_t(b)}\frac {\partial \sum^n_{c=1}e^{h_t(c)}}{\partial H_t(a))}}{(\sum^n_{c=1}e^{h_t(c)})^2}$ - quotient rule
$=\cfrac{\mathbb{I}<em>{a=b}e^{H_t(a)}\sum^n</em>{c=1}e^{H_t(c)}-e^{H_t(b)}e^{H_t(a)}}{(\sum^n_{c=1}e^{h_t(c)})^2}$ ($\frac {\partial e^x}{\partial x}=e^x$ and $\sum^n_{c=1}e^{H_t(c)}$ contains $e^{H_t(a)}$)
$=\cfrac{\mathbb{I}<em>{a=b}e^{H_t(b)}}{\sum^n</em>{c=1}e^{h_t(c)}}-\cfrac{e^{H_t(b)}e^{H_t(a)}}{(\sum^n_{c=1}e^{h_t(c)})^2}$
$=\mathbb{I}<em>{a=b}\pi_t(b)-\pi_t(b)\pi_t(a)$
$=\pi_t(b)(\mathbb{I}</em>{a=b}-\pi_t(a))$ $\square$</p>

<p>Gradient-based algorithms estimate action preferences, not values
Connected bandits = associative search tasks</p>
<h2 id="gittins-indices">Gittins Indices</h2>
<p>Provide optional solution to certain forms of bandit problems</p>

<h1 id="exercises">Exercises</h1>
<p><strong>Exercise 2.1</strong> 
In the comparison shown in Figure 2.1, which method will perform best in the long run in terms of cumulative reward and cumulative probability of selecting the best action? How much better will it be? Express your answer quantitatively.</p>

<p>$\epsilon=0.01$ will perform the best in the long run, as its average reward will eventually overtake $\epsilon=0.1$
This is because the chance of randomly choosing the suboptimal path is lower. It will end up choosing optimal actions 99% of the time.</p>

<p><strong>Exercise 2.2</strong> 
Give pseudocode for a complete algorithm for the $n$-armed bandit problem. Use greedy action selection and incremental computation of action values with $α = \frac 1 k$ step-size parameter. Assume a function bandit(a) that takes an action and returns a reward. Use arrays and variables; do not subscript anything by the time index t (for examples of this style of pseudocode, see Figures 4.1 and 4.3). Indicate how the action values are initialized and updated after each reward. Indicate how the step-size parameters are set for each action as a function of how many times it has been tried.</p>

<p>Initialise array $N=0$ of length number of actions
Initialise array $Q=0$ of length number of actions
A = $\arg\max_a$ Q[a]
Q[a]=Q[a]+$\frac 1 {N[a]}$(bandit(A)-Q[a])
N[a]=N[a]+1</p>

<p><strong>Exercise 2.3</strong> 
If the step-size parameters, $\alpha_k$, are not constant, then the estimate $Q_k$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of $α_k$?</p>

<p>$Q_{k+1}=Q_k+\alpha_k[R_k-Q_k]$
$=\alpha_kR_k+(1-\alpha_k)Q_k$
$=\alpha_kR_k+(1-\alpha_k)(Q_{k-1}+\alpha_{k-1}[R_{k-1}-Q_{k-1}])$
$=\alpha_kR_k+(1-\alpha_k)(\alpha_{k-1}R_{k-1}+(1-\alpha_k)(1-\alpha_{k-1})Q_{k-1})$
$=\alpha_kR_k+\alpha_{k-1}(1-\alpha_k)R_{k-1}+…+\alpha_1R_2(1-\alpha_k)…(1-\alpha_2)+(1-\alpha_k)…(1-\alpha_1)Q1$
$=Q_1\Pi^k_{i=1}(1-\alpha_i)+\sum^k_{i=1}(\alpha_iR_i\Pi^k_{j=1}(1-\alpha_j))$</p>

<p><strong>Exercise 2.4 (programming)</strong> Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the q(a) start out equal and then take independent random walks. Prepare plots like Figure 2.1 for an action-value method using sample averages, incrementally computed by α = 1 k , and another action-value method using a constant step-size parameter, α = 0.1. Use ε = 0.1 and, if necessary, runs longer than 1000 plays.</p>

<p><strong>Exercise 2.5</strong>
The results shown in Figure 2.2 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? What might make this method perform particularly better or worse, on average, on particular early plays?</p>

<p>Depends on how the original values are set. If they are more accurate early on, it will perform better</p>

<p><strong>Exercise 2.6</strong> 
Suppose you face a binary bandit task whose true action values change randomly from play to play. Specifically, suppose that for any play the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any play, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each play you are told if you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?</p>

<p>If you bet 1: $\mathbb{E}(A)=0.5(0.1)+0.5(0.9)=0.05+0.45=0.5$
If you bet 2: $\mathbb{E}(B)=0.5(0.2)+0.5(0.8)=0.1+0.4=0.5$
Expected value is the same, doesn’t matter which you choose
You have to explore the rewards of all cases</p>]]></content><author><name></name></author><category term="rl" /><category term="textbook" /><summary type="html"><![CDATA[Instructive Feedback Indicates correct action to take Independent of action taken Evaluative Feedback Indicates how good action taken is Depends entirely on action Associative Actions are taken in more than 1 situation AN $n$-Armed Bandit Problem Limited time steps Imagine an $n$-armed slot machine]]></summary></entry><entry><title type="html">Reinforcement Learning: An Introduction - Chapter 1</title><link href="http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html" rel="alternate" type="text/html" title="Reinforcement Learning: An Introduction - Chapter 1" /><published>2025-08-28T00:00:00+01:00</published><updated>2025-08-28T00:00:00+01:00</updated><id>http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1</id><content type="html" xml:base="http://localhost:4000/rl/textbook/2025/08/28/rl-chapter-1.html"><![CDATA[<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body,
	      {delimiters: [
	      {left: '$$', right: '$$', display: true},
	      {left: '$', right: '$', display: false},
	      {left: '\\(', right: '\\)', display: false},
	      {left: '\\[', right: '\\]', display: true}
	      ]});"></script>

<h1 id="rl">RL</h1>
<p>RL is focused on goal-directed learning
Learning how to map situations to actions to maximise the reward signal</p>
<h2 id="closed-loop">Closed Loop</h2>
<p>Actions influence later inputs</p>
<h2 id="rl-problem-requirements">RL Problem Requirements</h2>
<ol>
  <li>Closed-loop</li>
  <li>Not have direct instructions about what action to take</li>
  <li>Consequences of actions over extended periods of time</li>
</ol>

<p>RL tries to maximise reward signal instead of trying to find hidden structure
RL | Supervised Learning | Unsupervised Learning - they are distinct</p>
<h3 id="explorationexploitation-trade-off">Exploration/Exploitation Trade-off</h3>
<h1 id="subelements-of-rl-system">Subelements of RL system</h1>
<h2 id="1-policy">1. Policy</h2>
<p>Mapping from perceived state to actions</p>
<h2 id="2-reward-signal">2. Reward signal</h2>
<p>Defines the goal
Each time step, the environment sends to the reinforcement learning agent a single number, a <strong>reward</strong></p>
<h2 id="3-value-function">3. Value Function</h2>
<p>Specifies what is good in long run
Value of a state is the total reward expected in future from that state</p>

<p>Reward is immediate, primary | Value is long-term, secondary</p>
<h2 id="4-optional-model-of-environment">4. (optional) Model of environment</h2>
<p>Model of environment
Mimics environment and leads to predictions about the environment
Models enable planning, which is deciding on a course of action by considering possible future situations
Model-free is trial-and-error learning</p>
<h1 id="evolutionary-methods">Evolutionary Methods</h1>
<p>Non-learning (not learning from environment)
Finds and utilises best policy</p>

<p>| Pros                                                                                                                      | Cons                     |
| ————————————————————————————————————————- | ———————— |
| Works well when either:<br />1. Small set of policies<br />2. Good policies easy to find<br />3. Lots of time for policy search | Ignore useful structures |
| Have advantages when agent cannot sense state of environment                                                              |                          |</p>
<h2 id="policy-gradient-methods">Policy Gradient Methods</h2>
<p>Estimate direction parameters should be adjusted to most rapidly improve policy’s performance</p>

<h1 id="tic-tac-toe">Tic-Tac-Toe</h1>
<h2 id="assumptions">Assumptions</h2>
<p>Opposition player is imperfect and sometimes blunders</p>
<h2 id="consequences">Consequences</h2>
<p>Cannot use minimax, because that assumes the opponent is always correct</p>
<h1 id="value-update-equation">Value Update Equation</h1>
<p>$V(s)\leftarrow V(s) + \alpha[V(s’)-V(s)]$
$\alpha$ - Step size parameter</p>

<p>Converges if $\alpha$ reduces over time
If $\alpha$ remains $&gt;0$, it can adjust to an opponent that changes strategy over time</p>
<h1 id="problems-with-evolutionary-methods">Problems with Evolutionary Methods</h1>
<p>Value functions methods allow individual states to be evaluated
Evolutionary is less granular
Evolutionary holds policy fixed, which means that it attributes same value to all moves</p>
<h1 id="rl-history">RL History</h1>

<h2 id="3-threads-led-to-rl">3 Threads led to RL</h2>
<ol>
  <li>Trial + error learning</li>
  <li>Optimal control -  designing a controller to minimise measure of behaviour</li>
  <li>Temporal difference methods - difference between temporally successive estimates of same quantity
    <h1 id="curse-of-dimensionality">Curse of Dimensionality</h1>
    <p>Computational requirements grow exponentially with number of variables</p>
    <h1 id="animal-learning">Animal Learning</h1>
    <h2 id="law-of-effect">Law of Effect</h2>
    <p>Strength of feeling $\propto$ Strength of learning</p>
    <h2 id="secondary-reinforcer">Secondary Reinforcer</h2>
    <p>Stimulus that has been paired with a primary reinforcer such as food or pain
(Value, rather than reward)</p>
    <h1 id="credit-assignment-problem">Credit Assignment Problem</h1>
    <p>How do you distribute success for many decisions?</p>
    <h1 id="exercises">Exercises</h1>
    <p><strong>Exercise 1.1: Self-Play</strong> 
Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?</p>
  </li>
</ol>

<p>It would continually invent new strategies to improve, as long as the step size parameter $\alpha&gt;0$</p>

<p><strong>Exercise 1.2: Symmetries</strong>
Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that  symmetrically equivalent positions should necessarily have the same value?</p>

<p>Conduct rotations to reveal symmetries. Can have fewer paths to make and thus fewer decisions</p>

<p>Symmetrically equivalent positions should necessarily have the same values as long as the imperfect opponent makes the same blunders in those positions</p>

<p><strong>Exercise 1.3: Greedy Play</strong> Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Would it learn to play better, or worse, than a non-greedy player? What problems might occur?</p>

<p>Greedy player priorities reward over value, which is too short-term and will likely lose more often</p>

<p><strong>Exercise 1.4: Learning from Exploration</strong> 
Suppose learning updates occurred  after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?</p>

<p>When we do not learn from exploratory, the probabilities will match the ground truth probabilities.
If we learn from exploratory, we are also learning from random, likely sub-optimal moves, which will add random error to the probabilities. Better to learn from exploratory IF we never stop making exploratory moves, as it accounts for these future errors</p>

<p><strong>Exercise 1.5: Other Improvements</strong>
Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?</p>

<p>Initial values are set to 0.5, which is not accurate. For example, if opponent has 2 in a row, the probability of winning will be lower. Will learn faster as a result
Better ways to solve problem may be heuristic-based</p>]]></content><author><name></name></author><category term="rl" /><category term="textbook" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ProcrastApply</title><link href="http://localhost:4000/personal/blog/procrastapply/2025/07/11/procrastapply.html" rel="alternate" type="text/html" title="ProcrastApply" /><published>2025-07-11T00:00:00+01:00</published><updated>2025-07-11T00:00:00+01:00</updated><id>http://localhost:4000/personal/blog/procrastapply/2025/07/11/procrastapply</id><content type="html" xml:base="http://localhost:4000/personal/blog/procrastapply/2025/07/11/procrastapply.html"><![CDATA[<p>me
 — 
13/03/2025, 09:15
03/13/2025:
I am working on making installations easier.
In Python (what the application is written in), it is quite difficult to make executables for different OSs. I know it’s a hassle on Mac and will make a how-to guide as a temporary solution.</p>

<p>There now is a release for Linux,  shout out to all my Linux besties. However, even with this you need to do chmod +x ProcrastApply after installation to be able to run it. I’m trying to find better solutions. If you guys know anything please lmk</p>

<p>Am going to make daily updates to keep all of you in the loop about what progress is being made. If you have any comments please lmk
me
 — 
14/03/2025, 11:20
03/14/2025:
Relating to yesterday’s post, a how to guide for Linux:
Go to procrastapply.com
Set your filtersFill in your email and upload your resumeDownload the folderUnzip the folderRun the following:cd Downloads/ProcrastApply_Package
chmod +x ProcrastApply
./ProcrastApply</p>

<p>If you try this and it doesn’t work, let me know.
Again, I want to make this more seemless</p>

<p>Navbar
I created a navbar! Woo woo.
I’m aiming to implement a Google sign in. This way all UCs can use this, but I don’t need to manage any passwords in the future.</p>

<p>See you all tomorrow!
me
 — 
16/03/2025, 00:09
03/15/2025:
I know it’s a bit late, had a weirdly busy day. Anyways, as promised.
Created 3 new pages:
Homepage - Gives a bit more context about the page + ProcrastApply
How-to - Gives more info about how to install the application. Still trying to find a better methodPricing - FREE!</p>

<p>Plans for tomorrow: Google Sign in so that people don’t have to fill in their email.</p>

<p>See you all soon
me
 — 
17/03/2025, 01:06
03/16/2025:
I have a plan!</p>

<p>Downloading this is terrible. I’m sorry to everyone who has tried. I believe I can turn this into a web app to prevent all this hassle. This is going to take some work with AWS, so please bear with me. I am making this priority #1</p>

<p>GL with finals everyone!
me
 — 
17/03/2025, 23:41
03/17/2025:
That plan didn’t work!</p>

<p>I don’t want to install anything and I don’t think you do either. I’m instead now trying to make it a chrome extension. I still want to deliver massive value with our service and I don’t want to dilute this with making “just another chrome extension” – I want to make a great chrome extension, that you all actually like.</p>

<p>I’ll let you all know when it’s ready to go.</p>

<p>Hope you’re surviving the exam season!
me
 — 
19/03/2025, 00:36
03/18/2025:
It is slowly getting there. Having to re-implement nearly all of it in a different language. However, that is faster than writing it for the first time!</p>

<p>Hope finals aren’t too bad, nearly half way through!
me
 — 
20/03/2025, 00:33
03/19/2025:
Makin’ the Chrome extension. This might turn out really nice.</p>

<p>Hope you’re all ready for spring break!
me
 — 
20/03/2025, 23:23
03/20/2025
So close to end of quarter - which means more time for ProcrastApply! Having some difficulty with communicating between content.js and background.js If you guys know stuff about sending messages in chrome extensions lmk.
me
 — 
22/03/2025, 03:24
03/21/2025
Congrats on getting through finals! I know it’s been a hard journey. I am getting to grips with the messaging system in Chrome extensions. Hope you all have a wonderful break
me
 — 
22/03/2025, 17:22
03/22/2025
Still at it. Finally got the messages to work. Next step is to automatically apply!
me
 — 
23/03/2025, 22:49
03/23/2025
Getting the autosubmissions working again. It’s going to be complicated to get the cover letters aspect working, but I will get it!
me
 — 
24/03/2025, 22:16
03/24/2025
It is moving! The extension runs on Handshake now and should get the flow working.
Tomorrow
I want to get the bare minimum flow working (submit application)
me
 — 
25/03/2025, 23:52
03/25/2025
OK, didn’t get bare minimum flow working, got website opener to work. Will have to break this process up into baby steps
Tomorrow
Submit 1 application
me
 — 
27/03/2025, 00:31
03/26/2025
So bleeding close! Basic functionality works, but I want to keep track of the number of applications, which I will. CORS errors are induced, and who doesn’t hate CORS errors.
Tomorrow
Get basic API request -&gt; Simple job applications
me
 — 
27/03/2025, 23:38
03/27/2025
CORS errors = Gone!
Backend = Working!
Flow = Nearly there!</p>

<p>I want to get this right first time. No later date corrections. I want this to run smoothly and correctly. 
Tomorrow
Submit resume working
me
 — 
28/03/2025, 23:55
03/28/2025
Huge day! Got basic flow working. First few applications with the Chrome extension only have been submitted!
Tomorrow
Iterate over page postings
me
 — 
29/03/2025, 18:32
03/29/2025
I’ve gone backwards, will have to reuse what I had yesterday. For some reason it’s really annoying getting the input boxes to load. Will have a rethink</p>

<p>Tomorrow
Remove today’s changes. Get the input boxes to be read
me
 — 
30/03/2025, 23:34
03/30/2025
Bit of a slow day tbh, hopefully my brain gets back into it. Removed yesterdays changes, need to tinker more with waiting for page to load
Tomorrow
Get flow to work when modal loads, as that’s causing problems
me
 — 
01/04/2025, 01:34
03/31/2025
We are getting the Chrome extension published on the Chrome store!</p>

<p>This means that ProcrastApply will be in your guys’ hands soon enough!</p>

<p>Today, I managed to get more of the input boxes working, so that soon we will be able to deal with resumes, then the next step will be cover letters. Unfortunately, there is a lot of trial and error in this process, but everyday we are getting closer to a better product!
Tomorrow
I want to get the resume boxes working
me
 — 
02/04/2025, 00:41
04/01/2025
We can look at multiple posts, it closes when it asks for transcript BUT we cannot submit resumes … yet. It crashes when it tries to do this. We will get there
Tomorrow
@Gman and I will be at the MU handing out leaflets to more people to join this discord chat and understand people’s problems with Handshake AND I will be coding more, hopefully getting CVs to work gah!
me
 — 
02/04/2025, 23:06
04/02/2025
Diagnosed a lot of the issue when opening the modal. Am also getting the PDF library to work so that we can generate cover letters locally. 
Tomorrow
Make first PDF cover letter
me
 — 
04/04/2025, 01:44
04/03/2025
Woo! Made the first PDF, getting things moving.
Tomorrow
Need to make get our API request to work to get cover letter content
me
 — 
04/04/2025, 20:24
04/04/2025
Still trying to read the modal that pops out, having problems with getting the timing right. 
Tomorrow
Get paragraphs to work
Get at least 1 input box read 
me
 — 
05/04/2025, 23:30
04/05/2025
Got the paragraphs loaded in!
Tomorrow
Read input boxes
me
 — 
06/04/2025, 23:52
04/06/2025
Great day today! Applications now work with resumes (although not 100% consistently – am working on this).
Tomorrow
Want to get it consistent, day after tomorrow want to get cover letters fully working
me
 — 
07/04/2025, 22:54
04/07/2025
Bit of a slog today, struggling to get consistent applications to work. I’ve realised that getting the cover letters working is going to be more finicky than I anticipated – but it IS doable.
Tomorrow
Getting consistent applications
me
 — 
08/04/2025, 22:33
04/08/2025
Great day! Got the resumes working, turns out there was a much simpler method than what I was doing, much faster too.
Tomorrow
Commence work on the cover letter section. 
me
 — 
09/04/2025, 23:03
04/09/2025
Yup, so getting the cover letters going is slow work. I’m having trouble getting the next page to load afterwards, although I think it’s more to do with what’s going on with cover letters.
Tomorrow
Get background-&gt;content response 
me
 — 
11/04/2025, 00:26
04/10/2025
Gah! This is frustrating, getting the next page thingy to work. Just gotta focus on background-&gt;content which I said I would!
Tomorrow
Forget about next page for now, get the response to work when a cover letter is created.
me
 — 
11/04/2025, 09:02
04/11/2025
Getting there, all about getting the async calls just right.
Tomorrow
Picnic day!
Still gonna be grinding! Think I might have the next page button working, just need to get the cover letters working
me
 — 
12/04/2025, 15:37
04/12/2025
Happy picnic day!
I got cover letters working - can’t quite believe it! Now all I need to do is wait a short period before it submits application.
Tomorrow
Add a small timer before submission</p>

<p>Stay Safe!
me
 — 
13/04/2025, 22:26
04/13/2025
Getting the PDF text looking just right, need this for submissions
Tomorrow
Get PDFs perfect and need the submission to work
me
 — 
14/04/2025, 21:52
04/14/2025
Cover letter functionality is now WORKING!! WOO. This is excellent news, now I just need to get it to works so you can upload your resume and get the functionality just right
Tomorrow
Get storage and resume upload working
me
 — 
15/04/2025, 22:16
04/15/2025
Happy tax day! Got the flow to work when already applied to positions. Haven’t got resume upload to work yet.
Tomorrow
Upload the resume pdf at the least
me
 — 
16/04/2025, 13:34
04/16/2025
Handshake have updated their website. Which means that I have to rewrite many many lines of code. Woo!
Chrome extensions are cool though, you can locally host websites on them- WHAT!? Crazy
Tomorrow
Get pdf upload working and accessible
me
 — 
17/04/2025, 23:08
04/17/2025
Got the PDF upload to work. Started getting the flow working for the Handshake update. Handshake does look a lot cleaner if you lot haven’t checked it out yet.
Tomorrow
More updates for the flow. Want to get cover transcripts working, and then cover letters
me
 — 
18/04/2025, 14:57
04/18/2025
Cover letters are working, surprise how quickly I got that working.
Tomorrow
Get resume text read into cover letters so it’s customised
me
 — 
19/04/2025, 18:43
04/19/2025
PDFs are uploaded, struggling to re-read them for the cover letter section.
Tomorrow
Get PDF lib to work
me
 — 
20/04/2025, 23:03
04/20/2025
Happy Easter!</p>

<p>I am getting close to getting the PDFs read. This week will be exciting. Hopefully you can all download it and get some actual value from it!</p>

<p>Tomorrow
PDF Read!
me
 — 
21/04/2025, 23:02
04/21/2025
PDFs are being read! Things are looking good, now I just need the next items to consistently be read. Looking forward to seeing you lot soon!
Tomorrow
Skip the correct postings
me
 — 
22/04/2025, 22:21
04/22/2025
Cover letters now work! Very exciting times. Now all that’s remaining on the user end is a weird bug that resets when it reaches the 4th post.
Tomorrow
Continue reading all posts on the page</p>

<p>Your guys’s HW:
Download the extension!
https://chromewebstore.google.com/detail/procrastapply/bhagaimmcfnbbdoffdoegknlacgponcp
ProcrastApply - Chrome Web Store
Getting students better jobs without the busywork
Image
me
 — 
23/04/2025, 23:47
#04/23/2025
Found the bug causing post resets - nested loops and repeatedly using the variable i - a classic! This now means it’s working!! Going to the next page is a bit funky, but that shouldn’t take too long.</p>

<p>Looking to seeing you all on Friday!
Tomorrow
Fixing the next page issue</p>

<p>Download link:
https://chromewebstore.google.com/detail/procrastapply/bhagaimmcfnbbdoffdoegknlacgponcp
ProcrastApply - Chrome Web Store
Getting students better jobs without the busywork
Image
me
 — 
25/04/2025, 00:18
04/24/2025
Got next page to work! Am now making the filtering page much nicer.
Tomorrow
Get page filtering to work and give you all some free pizza 😋
me
 — 
25/04/2025, 21:10
04/25/2025
Today we gave out $200 of pizza– hope everyone who got a slice enjoyed it! 🍕
Also managed to get Gmail login working. This should work for all UCs and will cater to the different Handshake domains.
Tomorrow
Will start job filtering stuff, it’s quite menial but oh well.
me
 — 
26/04/2025, 23:07
04/26/2025
Next page is working. Filtering page looks better. Now filters need to be implemented.
Tomorrow
Filters baby
me
 — 
28/04/2025, 00:35
04/27/2025
Working on filters. Less boring than I thought it would be!
I know people are facing issues on their Macs with the application. I will get to the bottom of this, and will update you all.
Tomorrow
More filters woo woo
me
 — 
28/04/2025, 23:41
04/28/2025
So, it turns out that Handshake has updated their website for some users but not all. This is a problem, as I cannot develop for both platforms. We have tried getting in touch with them and will likely have to use a few connections to get in touch. 
That being said, did some good work with the filters today + cleaned up logo!
Tomorrow
More filters stuff. Hopefully get an ETA for when Handshake updates website.
me
 — 
29/04/2025, 23:25
04/29/2025
Worked on the website today. Feel like I’m twiddling my thumbs waiting for Handshake getting back to me. We need to make this happen immediately.
Tomorrow
Work on signing up for ProcrastApply on the Chrome extension
me
 — 
30/04/2025, 22:52
04/30/2025
Got the background to start working again. Need to look at some backend issues, currently it’s returning “Forbidden” - Yikes! 
Tomorrow
Sign up functionality on the backend + Cover letter stuff (I suspect it’s an issue with ChatGPT things)
me
 — 
01/05/2025, 10:33
05/01/2025
Pinch, punch, first day of the month!</p>

<p>Got the user creation done, got in the flow this fine morning. The product works… However, you lot can’t access it - that sucks! I could really do with the user feedback rn. Hopefully Handshake gets back to us.
Tomorrow
Create a demo so that you can understand how this works. The flow needs to be as smooth as possible.
me
 — 
02/05/2025, 09:03
05/02/2025
The demo video should be up soon when the Chrome webstore approves it! I have been thinking a little bit about security and will make a few revisions to ensure a safe authentication by users.
Tomorrow
Work on authentication 
me
 — 
03/05/2025, 20:49
05/03/2025
Working on the Google application now. AWS Layers are such a pain, surely Amazon could come up with a better way of doing this!
Might also revert some changes back to support the un-updated version of the Handshake website. I’m putting it off because it feels dumb and makes the work I’ve done pointless, but I am just going to have to bite the bullet.
Tomorrow
Get Google authentication finished
me
 — 
04/05/2025, 22:17
05/04/2025
Spent wayyyy too long getting the AWS layers sorted. At least I now know how to do it. Got a midterm next week so hope that also goes well. We have irons in the fire and will keep you all in the loop regarding a possible next step in ProcrastApply’s future.
Tomorrow
Make sure email check works - need to do a final thing with Google to get backend to work
me
 — 
06/05/2025, 00:03
05/05/2025
Midterm tomorrow ah! This Google stuff is a pain. Hoping to get it done tomorrow. Just messing around with it right now. At least I don’t appear to be the only one with this problem.
Tomorrow
Get. Authentication. Done 
me
 — 
07/05/2025, 01:13
05/06/2025
Ughh, still working on it
Tomorrow
Get manifest.json key working
me
 — 
07/05/2025, 13:35
05/07/2025
Woo! I found a cheeky way of doing it on the backend. Now that works thank goodness. Good luck to all of our friends presenting at Demo day today!
Tomorrow
Look at feasibility of storing more things on the DB 
me
 — 
09/05/2025, 04:00
05/08/2025
Got a lot of backend stuff working. Tomorrow is the day I get stuff working on Mac. Not waiting for Handshake to get back now.
Tomorrow
Get Mac working!
me
 — 
10/05/2025, 04:21
05/09/2025
Mac is not working. I can’t test to see if it will work. Messaged another Handshake employee. Starting another possible avenue to help you all get jobs.
Tomorrow
Have a long think about what is most worth doing
me
 — 
11/05/2025, 02:52
05/10/2025
Ok, this has been interesting. Working more on a new idea we’ve had to increase value for you and improve your adds of getting jobs. More details soon.
Tomorrow
Keep working on this
me
 — 
11/05/2025, 23:46
05/11/2025
Looking at new technologies. Looking to implement GraphQL for new project and am slowly getting to grips with it. Don’t want to be too hasty.
Tomorrow
Build GraphQL
me
 — 
12/05/2025, 23:47
05/12/2025
Turns out AWS is great at hosting mobile applications as well (hint hint). Amplify + Appsync + DynamoDB makes life very easy.
Tomorrow
Start building the thing 
me
 — 
14/05/2025, 04:07
05/13/2025
Having fun again! Put the app on Amplify and it runs like a dream. Want to start getting the functionality on it… This could be good
Tomorrow
Get barebones interface on it
me
 — 
14/05/2025, 23:57
05/14/2025
Hosting the app now, and immediately have authentication working. Not sure if messaging is functionality we would want. Will have to discuss it.
Tomorrow
Start working on shared similar interests. Build DB on information
me
 — 
16/05/2025, 00:39
05/15/2025
Improved the app. Spoke to someone in design about it. Not sure if we really are solving a problem but only 1 way to find out ig.
Tomorrow
Keep the ball rolling
me
 — 
17/05/2025, 02:06
05/16/2025
Made a bunch of improvements. Got user profiles looking better and the user flow better.
Tomorrow
Get user sign out looking better and improved  user interface. Need to look more at what students can do to differentiate themselves. 
me
 — 
17/05/2025, 22:42
05/17/2025
Slowly making page look much better. I feel like I need to get more substantial functionality. Perhaps getting more backend stuff working. Will have to see.
Tomorrow
Get some backend stuff working
me
 — 
19/05/2025, 00:37
05/18/2025
Making back end stuff. Need to interact with GraphQL from React Native side. Interesting getting it all together.
Tomorrow
More GraphQL stuff, try and get messages working
me
 — 
20/05/2025, 00:34
05/19/2025
Working on an interface for text autocomplete. LLM autocomplete might be an interesting functionality. Experimenting.
Tomorrow
Get it to work with an API?
me
 — 
21/05/2025, 01:08
05/20/2025
Cleaned the interface up, the text suggestions were pretty ugly. Getting irritating errors with the API key.
Tomorrow
Make the first API request
me
 — 
21/05/2025, 23:32
05/21/2025
Looking better, suggestion is better. Now got it working so that the text rolls around the side. Worried about abusing my API credits, so will try to make it only make a suggestion after a delay, or perhaps at specific moments in time.
Tomorrow
Test out API (with thought)
me
 — 
22/05/2025, 23:45
05/22/2025
Been cooking an idea up. Might actually be quite helpful… Will speak to the Gavdog abt it. Time to bring value to you lot.
Tomorrow
Hook up a backend?
me
 — 
24/05/2025, 21:55
05/23/2025
Apologies for delay, did it at 1:30. Making it look real good, can now add new users to the tables with their data.
Tomorrow
Add user persistence
05/24/2025
User persistence now works! But locally, maybe I’ll keep it super basic before making a back end and get people to test it before releasing. Can also edit users.
Tomorrow
Photos!
me
 — 
26/05/2025, 11:42
05/25/2025
Photos done! Next to do is filtering the items and returning the specific contact details
Tomorrow
Return phone numbers 
me
 — 
27/05/2025, 00:02
05/26/2025
Reach out functionality is much better and works with phones. Would be interesting to get user feedback, think I will ask people on Thursday.
Tomorrow
Entries should have control over being in the database (I think)
me
 — 
28/05/2025, 01:22
05/27/2025
Hello all!
Patched up a few things so that new users can be added again. Need to prep the demo for Thursday, this is also a bit of a mad week so need to get everything done. Regarding next functionality, not really sure. Will discuss with G tomorrow.
Tomorrow
Discuss with G and then make stuff from that
me
 — 
29/05/2025, 01:55
05/28/2025
Met with G. Had a good talk. We have direction now and now what we want. We also know what we’re up against now.
Tomorrow
Build site (know what we want, will divulge details later)
me
 — 
30/05/2025, 01:16
05/29/2025
Building the website some more, facing issues with resume upload in NextJS.
Tomorrow
Busy day! Get resume upload working
me
 — 
30/05/2025, 23:54
05/30/2025
Resume upload is now working! Need to get the match suggestions working next.
Tomorrow
Match suggestions
me
 — 
31/05/2025, 14:30
05/31/2025
Trying a slightly different format today. I’m going to start by setting my intentions in my work and then seeing if I met them.
Intention
Get sample email functionality working so that it can quickly be sent to the target contacts.
Result
Got this working. I need to spend some time planning the flow.</p>

<p>first time users: Upload resume -&gt; getMatches -&gt; createEmail</p>

<p>nth time users: resume already uploaded -&gt; getMatches -&gt; createEmail</p>

<p>Tomorrow
Get this flow to work, using useEffect
me
 — 
02/06/2025, 00:38
06/01/2025
Intention
Get following flow to work:</p>

<p>first time users: Upload resume -&gt; getMatches -&gt; createEmail</p>

<p>nth time users: resume already uploaded -&gt; getMatches -&gt; createEmail
Result
Flow is working. Now need to clean up displaying resume.
Tomorrow
Display resume cleanly
me
 — 
02/06/2025, 10:46
06/02/2025
Intention
Display resume cleanly
Result
Resume is not cleanly displayed, been focusing on getting the email functionality to work with requests to OpenAI - useState + useEffect weirdness is annoying. Got it working now.
Tomorrow
Display resume text - need to think of something else exciting to do as well
me
 — 
04/06/2025, 01:11
06/03/2025
Intention
Display resume text
Turns out it’s also making a BUNCH of API calls - this ain’t good, would spam my backend
Result
Ugh, trying to render this PDF, but it’s not working. Need to try some more attempts
Tomorrow
Get this PDF visible and get the text from it
me
 — 
05/06/2025, 01:48
06/04/2025
Intention
Get this PDF visible and get the text from it
Result
PDF is visible!
There is a thing about using sensitive personal data before utilising an LLM API - these things do not mix well. I want to ensure that there is a standard I am reaching that works.</p>

<p>Why isn’t there a label on websites that says something like - “WE ARE ISOXXX CERTIFIED”? Feel like that would be a good idea and circumnavigate those annoying popups.
Tomorrow
Redact sensitive data
me
 — 
06/06/2025, 01:42
06/05/2025
Intention
Redact sensitive data
Result
Weirdly, no good solution exists. I don’t know why this is.</p>

<p>I guess I’m going to have to go on a little sidequest now to create a light-weight redaction model.</p>

<p>There are some that already exist: https://huggingface.co/strickvl/nlp-redaction-classifier</p>

<p>However, I don’t know if it works for everything I want (names, addresses, emails, phone #s). Could be a small little tid-bit. I’m very surprised this doesn’t exist already 
Tomorrow
Fine-tune that model!
strickvl/nlp-redaction-classifier · Hugging Face
strickvl/nlp-redaction-classifier · Hugging Face
me
 — 
06/06/2025, 17:43
06/06/2025
Intention
Fine-tune that model!
Result
Settled on this model:
https://huggingface.co/Isotonic/deberta-v3-base_finetuned_ai4privacy_v2</p>

<p>Runs pretty well and seems quite popular. Only thing is it splits up the words by tokens. This means I have funky behaviour where I will need to combine the tokens together in order to successfully redact sensitive info.
Tomorrow
Combine tokens together and get redacted resume text - This will be pretty cool to see
Isotonic/deberta-v3-base_finetuned_ai4privacy_v2 · Hugging Face
Isotonic/deberta-v3-base_finetuned_ai4privacy_v2 · Hugging Face
me
 — 
08/06/2025, 00:34
06/07/2025
Intention
Combine tokens together and get redacted resume text - This will be pretty cool to see
Result
It is nearly working! Pretty cool to see it redact the sensitive information. Problem is when there are multiple sensitive words that need to be removed due to index errors 
Tomorrow
Solve these index errors
me
 — 
08/06/2025, 10:41
06/08/2025
Intention
Solve these index errors
Result
I have nearly solved these indexing issues. It’s more of a fun coding problem than I thought. Screw ChatGPT, I’m enjoying actually using my brain to solve a classic programming problem. Took longer than anticipated due to lack of logging (I’ve since fixed)
Tomorrow
Get my solution finished.
me
 — 
09/06/2025, 22:50
06/09/2025
Intention
Get my solution finished.
Result
Fixed! Did some more stuff to remove some forms of filtering. This is actually quite a nice solution
Tomorrow
Get OpenAI API up
me
 — 
11/06/2025, 03:52
06/10/2025
Intention
Get OpenAI API up
Result
Did not do this. I instead worked on getting the PDF viewable - which it now is. There were some weird funky things with local storage and have subsequently moved to indexedDB which I did not even know existed.
Tomorrow
Sleep. Then get OpenAI API up n’ runnin’</p>

<p>GL everyone with finals!
me
 — 
11/06/2025, 14:38
06/11/2025
Intention
Sleep. Then get OpenAI API up n’ runnin’
Result
Oops, did not sleep very much and had a massive brain wave!</p>

<p>This is THE idea. GG and I are going to Stanford this summer and think that we have found a way to deliver MASSIVE value to students. I hope you are all excited as we are.
Best of luck with any more exams
Tomorrow
User creation - GG will be on data collection I believe
me
 — 
13/06/2025, 03:22
06/12/2025
Intention
User creation - GG will be on data collection I believe
Result
User creation is getting there, spent some time making things look good and adding extra functionality such as a bio section
Tomorrow
Get bio display to work
User update to work
me
 — 
14/06/2025, 04:17
06/13/2025
Intention
Get bio display to work
User update to work
Result
Not working. Quite late and v tired. Just trying to get stuff shipped.</p>

<p>User updates is closer to working.
Tomorrow
Get the user update to work -&gt; Get bio display up and running
me
 — 
14/06/2025, 23:57
06/14/2025
Intention
Get the user update to work -&gt; Get bio display up and running
Result
Done! Also got photo upload to work
Tomorrow
Make the contact look smarter</p>

<p>Make the colour scheme more red
me
 — 
16/06/2025, 00:36
06/15/2025
Intention
Make the contact look smarter
Make the colour scheme more red
Result
Contact looks a little better. 
Red is pretty ugly but at least I have a better CSS structure
Tomorrow
Think about linking people they have worked with and SUGGESTED people.</p>

<p>Consequently, will also have to start thinking about how papers connect people -&gt; Maybe even want a way of opening up a page with the list of authors on a paper.</p>

<p>Need to also speak with students about what they are looking for. Not sure which is the most important:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Recognition
</code></pre></div></div>

<p>Students in their labCitationsCollaborators
me
 — 
17/06/2025, 00:52
06/16/2025
Intention
Think about linking people PhDs have worked with and SUGGESTED people.</p>

<p>Consequently, will also have to start thinking about how papers connect people -&gt; Maybe even want a way of opening up a page with the list of authors on a paper.</p>

<p>Need to also speak with students about what they are looking for. Not sure which is the most important:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Recognition
</code></pre></div></div>

<p>Students in their labCitationsCollaborators
Result
Unfortunately, did not get a chance to speak with PhD’s, hopefully should be happening tomorrow but will see.</p>

<p>Did some basic stuff - not super productive, quite a stressful day.
Tomorrow
Has the potential to be terrible. Will lyk
me
 — 
18/06/2025, 00:40
06/17/2025
Intention
Has the potential to be terrible. Will lyk 
Result
Things still up in the air, should be OK
Regarding this stuff. Trying to setup a database to get stuff working. Looking at implementing OpenSearch - just familiarising myself</p>

<p>Tomorrow
Run an OpenSearch query
me
 — 
19/06/2025, 00:05
06/18/2025
Intention
Run an OpenSearch query
Result
I am sick. This has been hard. Trying to get through the week rn, but want to deliver. Every. Day.
Created OpenSearch gonna go sleep now. Hope everyone is having a good summer.
Tomorrow
Do more! Please! I don’t like being ill 😦
me
 — 
20/06/2025, 00:50
06/19/2025
Intention
Do more! Please! I don’t like being ill 😦
Result
Feeling much better!
Made a small change to make the URL available so that this works better for web.
Tomorrow
Take the leap and just start with a solution for making a searchable platform
me
 — 
21/06/2025, 02:07
06/20/2025
Intention
Take the leap and just start with a solution for making a searchable platform
Result
OK, turns out search is really quite complicated. I’m gonna keep this as easy and simple as I can and focus on user login and sign up - then at least stuff starts making sense. Search is kinda secondary.
Tomorrow
Make a front facing first page that has the search part. Then also have user login + signup / see profile buttons and pages
me
 — 
22/06/2025, 02:19
06/21/2025
Intention
Make a front facing first page that has the search part. Then also have user login + signup / see profile buttons and pages
Result
Front facing part was easy.
Now need to have better login + signup functionality. Need to see when a user is actually signed in and what their user id would be - match Cognito with db of other stuff.
Tomorrow
Look at AWS ways to match cognito things with full apps
me
 — 
22/06/2025, 18:37
06/22/2025
Intention
Look at AWS ways to match cognito things with full apps
Result
Using guide: https://medium.com/@johnelisaaa/building-a-full-stack-react-app-integrating-aws-cognito-lambda-rds-mysql-amplify-api-a4f616559acb</p>

<p>Setting up MySQL on my device. Have never managed DBs before so this is a fun (and also painful) new learning experience. Trying to connect to my VPC.
Tomorrow
Connect to VPC
Medium
Building a Full Stack React App: Integrating AWS Cognito + Lambda +…
In this tutorial, we’ll build a secure REST API for our e-commerce website by integrating AWS Cognito, AWS Lambda, and MySQL (RDS).
Building a Full Stack React App: Integrating AWS Cognito + Lambda +…
me
 — 
23/06/2025, 23:47
06/23/2025
Intention
Connect to VPC
Result
Set up mySQL on my device and made tables. Also set up AWS db which is now synced to the users I have with AWS Cognito. Scared of screwing something up, but so far seems pretty cool! Need to run some tests
Tomorrow
Make a first user on the DB!
me
 — 
25/06/2025, 01:54
06/24/2025
Intention
Make a first user on the DB!
Result
Have not made a user, had too late a day - GAH! Doing research takes up a lot of my day
Tomorrow
Make a damn user
me
 — 
25/06/2025, 23:45
06/25/2025
Intention
Make a damn user
Result
I did a thing. Used Replit to see if I could make the application. Did it superrrrr quick - scarily so. Issue is I’m now not familiar with the codebase, which I suppose is the point. Anyways. 
Ta-dah: https://0e575a98-43ae-435e-a727-06ce709d1fec-00-25zds93xmkkwt.worf.replit.dev/
Tomorrow
Keep workin’ it
me
 — 
27/06/2025, 00:44
06/26/2025
Intention
Keep workin’ it
Result
Holy cow! Replit is amazing. We have an MVP already, getting PhDs to use it. This is nuts!
Tomorrow
Get user feedback and improve it - software engineering is dead. All hail AI.
me
 — 
28/06/2025, 00:55
06/27/2025
Intention
Get user feedback and improve it - software engineering is dead. All hail AI.
Result
Got user feedback, implementing it right now. This is looking good. Did cost me $8 and I’m not sure about scaling it with DB. That is probably a later consideration though – tradeoffs!
Tomorrow
Implement more of the user feedback
me
 — 
29/06/2025, 01:20
06/28/2025
Intention
Implement more of the user feedback</p>

<p>Result
Have implemented more of the user feedback. I do get the sense that we’re not actually solving a problem and this is a strong fear of mine. More feedback we got today was that weak connections are not that helpful. In academia, network is very powerful, but it must be one of strong connection as research collaboration is predicated on people being of high calibre and trustworthy. These things take time. That being said, I do think there is value in a quickly shareable profile page so that people can follow you and your work over time. We shall see.
Tomorrow
Get more feedback. Good thing we’re seeing PhDs every day!
me
 — 
30/06/2025, 00:34
06/29/2025
Intention
Get more feedback. Good thing we’re seeing PhDs every day!
Result
In classic adolescent style, we’ve been chasing a solution and not a problem. The world has changed, and building software products is easier than ever. I’ve been doing it over the past 6 months to become a better coder and gain more insight, but these skills are less valuable than ever (IMO and exempting highly-specialised niches). Now we need to find a problem and get to grips with it. I like PhDs, they are mostly nice people. I want to make things that solve their problems. Now I need to understand what their problems are. We’re doing research now. Not building, even though I always have an itch to build, ultimately, the next skill I need to learn is patience and thinking before doing.
Tomorrow
Read more papers + articles about PhD problems.
me
 — 
01/07/2025, 00:54
06/30/2025
Intention
Read more papers + articles about PhD problems.
Result
Reading more. I’ve been given a very niche problem by a PhD and overlooked it. I think there is more to the story than I think. Could be that there is a very easy solution but remains unclear - will ask tomorrow.
Tomorrow
Ask PhD why they haven’t fixed a minor inconvenience
me
 — 
02/07/2025, 00:14
07/01/2025
Intention
Ask PhD why they haven’t fixed a minor inconvenience
Result
Read another paper about trouble PhDs face and spoke to a PhD today about their issue. It’s a problem of time. Their page is of low priority and so they don’t want to go change it.
Tomorrow
Having a conversation with a PhD about the application stuff.
me
 — 
03/07/2025, 02:01
07/02/2025
Intention
Having a conversation with a PhD about the application stuff.
Result
Discussion about PhD application stuff was good. Sounds like I probably want to do one. Anyways, there are a lot of things with the application process. I’m not sure if I can solve the things of or who is facing the problems. Sounds like it’s just generally a tough gig. Regardless will keep looking. Going on a break this week and hoping for some time to reflect. 
Tomorrow
Enjoy the sun!
me
 — 
04/07/2025, 01:09
07/03/2025
Intention
Enjoy the sun!
Result
Enjoyed the sun! Spent some more time reading docs about challenges of PhD. It might have to happen.
Tomorrow
Happy 4th July! Keep reading!
me
 — 
05/07/2025, 03:32
07/04/2025
Intention
Keep reading!
Result
Hope everyone had a great 4th of July! Had to squeeze some stuff in now after a fun day out. Really need to discuss future of this project. Don’t have a solid problem yet and reading the information out there is quite generalised. There is something else I have been thinking about, but it’s not relevant. Hoping that having a small break from things will help.
Tomorrow
Keep things percolating and try to rest up (it’s late!).
me
 — 
06/07/2025, 01:49
07/05/2025
Intention
Keep things percolating and try to rest up (it’s late!).
Result
Had a talk with the Gman. Decided to keep going with the PhD stuff. We are actually bringing value, but the problem is conflicting motivations - they want the perks of being well connected with an UTD website, but they don’t want to maintain it. Therefore, we’re maintaining it for them - that’s it, we’re doing just that. Gman and I are updating the websites so that they don’t have to worry. We’re doing stuff that won’t scale. 
Tomorrow
Begin the grind of updating the websites
me
 — 
07/07/2025, 01:20
07/06/2025
Intention
Begin the grind of updating the websites
Result
I have begun the grind of creating profiles and am now up to 9 (it takes a fairly long time to be thorough). This is a tenet of doing things that don’t scale. If we begun by doing this stuff by hand, we will begin to learn how to do this in a manner that begins to scale. We should do this the dumb way so that we can begin to do it a bit smarter. I think for the time being we just keep doing this until something clicks.
Tomorrow
Fix the paper view on the personal pages themselves.
Think about adding more collaborators.
me
 — 
08/07/2025, 00:47
07/07/2025
Intention
Fix the paper view on the personal pages themselves.
Think about adding more collaborators.
Result
Had a great talk with a mentor today. We need to redefine and refocus what we are doing. Started back up on the alumni connection stuff. Was very easy to build back up. Gonna show the Gman how to do stuff on Replit - it is truly amazing.
Tomorrow
Show Gman Replit
me
 — 
Yesterday at 01:16
07/08/2025
Intention
Show Gman Replit
Result
Did not get to show the Gman. Will hopefully do that tomorrow when together. Did start looking at collating data for our project. We are gonna need a better way of doing this. Will likely take getting in touch with someone. Maybe we do just reach out to Davis.
Tomorrow
Show the Gman replit. Discuss reaching out to Davis.
me
 — 
00:13
07/09/2025
Intention
Show the Gman replit. Discuss reaching out to Davis.
Result
Showed Replit to the Gman. It’s a pretty amazing technology and reckon everyone will be using it. It does cost money but I think it’s one of those moments where you have to pay to play with the latest stuff. Genuinely hours of coding saved by using it. There definitely are still gaps which AI cannot solve - primarily the messiness of human beings. With that said, I’m a huge proponent. Let’s see what else it can do! I should start using cursor as well - that’s free for students!
Need to send an email to Davis
Tomorrow
Download Cursor.
Email Davis.
me
 — 
00:09
07/10/2025
Intention
Download Cursor.
Email Davis.
Result
Cursor is amazing.
Davis warm connection inbound.</p>

<p>With that said, this might be goodbye – from me, not ProcrastApply.</p>

<p>I have made daily updates for the past 4 months and it has now reached a point where I say farewell. This has been a journey for which I am extremely grateful and have loved. We have met great people, learnt great lessons and made great mistakes; I couldn’t have asked for more.</p>

<p>Unfortunately, my time in the US and with you guys must come to an end.</p>

<p>Total days worked on project: 183
Total blog posts written: 120
Total spent on AWS: $13.17
Total spent on pizza: $200
Total spent on domains: $12.17
…
Total Cost: PRICELESS!!!</p>

<p>I wish you all the best and leave you in the capable hands of the Gman. ♥️</p>

<p>Tomorrow
Something else, not better or worse, just different.</p>]]></content><author><name></name></author><category term="personal" /><category term="blog" /><category term="procrastapply" /><summary type="html"><![CDATA[me — 13/03/2025, 09:15 03/13/2025: I am working on making installations easier. In Python (what the application is written in), it is quite difficult to make executables for different OSs. I know it’s a hassle on Mac and will make a how-to guide as a temporary solution.]]></summary></entry><entry><title type="html">First Public Post</title><link href="http://localhost:4000/personal/blog/2025/06/02/first-post.html" rel="alternate" type="text/html" title="First Public Post" /><published>2025-06-02T07:49:53+01:00</published><updated>2025-06-02T07:49:53+01:00</updated><id>http://localhost:4000/personal/blog/2025/06/02/first-post</id><content type="html" xml:base="http://localhost:4000/personal/blog/2025/06/02/first-post.html"><![CDATA[<p>First ever public post. Felt like it was time to. I have experienced positives of posting things on the Internet, it’s an amazing thing putting things out into the Universe. As long as I’m outputting stuff into the world, things seem to happen.</p>

<p>That sounds well and good, but my innate skepticism comes out and would say that sounds braggadocious and pretentious. Maybe this is really a platform for me to vent my thoughts and see if the Internet is an echo chamber for other people to agree with me and amplify my narcissism. Irrespective, this feels like an honest medium where I can relay my thoughts and experiences.</p>

<p>I’ve been making daily code updates for my side project ProcrastApply since the 13 March 2025 and will upload all of those posts.</p>

<p>The thing that pushed me to make the blog today is a deep distrust of ChatGPT. I’m making a website where I need to upload a PDF. I don’t know whether to parse it on the frontend or backend. My thoughts: frontend could be slow, backend PDF parsing sounds like a security nightmare. ChatGPT recommended I do it on the backend because ‘security’?? Doesn’t make sense. I’m familiar with the ELIZA effect and people ascribing ChatGPT and similar LLMs an authoritys status and I’m wary of it. I also feel like I’m losing an element of my faculty by using it for tasks such as these, but it is soooo much quicker.</p>]]></content><author><name></name></author><category term="personal" /><category term="blog" /><summary type="html"><![CDATA[First ever public post. Felt like it was time to. I have experienced positives of posting things on the Internet, it’s an amazing thing putting things out into the Universe. As long as I’m outputting stuff into the world, things seem to happen.]]></summary></entry><entry><title type="html">Novel AI Lexicon</title><link href="http://localhost:4000/ai/safety/2025/06/02/ai-safety.html" rel="alternate" type="text/html" title="Novel AI Lexicon" /><published>2025-06-02T07:49:53+01:00</published><updated>2025-06-02T07:49:53+01:00</updated><id>http://localhost:4000/ai/safety/2025/06/02/ai-safety</id><content type="html" xml:base="http://localhost:4000/ai/safety/2025/06/02/ai-safety.html"><![CDATA[<p>We need a new lexicon for AI. The anthropomorphisation of this technology is especially dangerous as it is causing emotional response to it. There are already papers citing the view of AI as an authority figure, yet also as a friend who is doing them a favour, which thus engenders the use of “please” and “thank you” A novel vocabulary must exist to prevent this danger.</p>

<p>Possible alternatives words:</p>

<p>Think - 
Believe - 
Says -</p>]]></content><author><name></name></author><category term="ai" /><category term="safety" /><summary type="html"><![CDATA[We need a new lexicon for AI. The anthropomorphisation of this technology is especially dangerous as it is causing emotional response to it. There are already papers citing the view of AI as an authority figure, yet also as a friend who is doing them a favour, which thus engenders the use of “please” and “thank you” A novel vocabulary must exist to prevent this danger.]]></summary></entry></feed>